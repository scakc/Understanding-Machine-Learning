{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familier with following concepts you can move to next notebook:\n",
    ">Reference http://www.deeplearningbook.org/contents/mlp.html\n",
    "\n",
    "    1. Example: Learning XOR\n",
    "    2. Gradient-Based Learning\n",
    "    3. Hidden Units\n",
    "    4. Architecture Design\n",
    "    5. Back-Propagation and Other Diﬀerentiation Algorithms\n",
    "    6. Historical Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep feedforward networks**, also called **feedforward neural networks**, or **multilayer perceptrons(MLPs)**, are the quintessential deep learning models.\n",
    "\n",
    "A feedforward networkdeﬁnes a mapping ***y=f(x;θ)*** and learns the value of the parameters θ that result in the best function approximation. There are no feedback connections in which outputs of the model are fed back into itself. When feedforward neural networks are extended to include feedback connections, they are called **recurrent neural networks.**\n",
    "\n",
    "A feedforward network consist of :\n",
    "   - Input Layer: where the input is given.\n",
    "   - Hidden Layer: where computation takes place and inputs and outputs are not of concern from obeservation point of view.\n",
    "   - Output Layer: where we expect the output. \n",
    "   \n",
    "<img src = 'dffn/ffn.png'>\n",
    "   \n",
    "here w's are called weights (parameters θ's) multiplied to inputs.\n",
    "   \n",
    "To extend linear models to represent nonlinear functions of x, we can apply the linear model not to x itself but to a transformed input φ(x), where φ is a nonlinear transformation. Equivalently, we can apply the kernel trick described in basic machine learning, to obtain a nonlinear learning algorithm based on implicitly applying the φ mapping. We can think of φ as providing a set of features describing x, or as providing a new representation for x.\n",
    "\n",
    "The question is then how to choose the mapping φ.\n",
    "\n",
    "   1. One option is to use very generic φ like RBF but generalization to test set often remains poor due to more complexity and careful prior choosing is required. \n",
    "   2. Another option is to manually engineer the φ which require a lot of analysis and time.\n",
    "   3. The strategy of Deep learning to learn φ. In this we have a model y = φ(x,θ) where we learn θ and φ is defined by all the hidden layers in the network but is unknown to us. The advantage is that the human designer only needs to ﬁnd the right general function family rather than ﬁnding precisely the right function.\n",
    "   \n",
    "Hidden Layers perform **activation function(S)** on the inputs it gets and returns the transformed output. These activation function contribute alot in learning process the non-linearity of φ etc.\n",
    "\n",
    "> read this : https://en.wikipedia.org/wiki/Feedforward_neural_network\n",
    "\n",
    "--------------\n",
    "\n",
    "### 1. Example: Learning XOR\n",
    "XOR is a logical operation with input and output set as:\n",
    "\n",
    "Logic XOR (Exclusive OR):<br>\n",
    "`\n",
    "Input  Output\n",
    "0 0    0\n",
    "1 0    1\n",
    "0 1    1\n",
    "1 1    0\n",
    "`\n",
    "\n",
    "Our model provides a function y=f(x;θ) which will be optimized to reach true function f*(x)\n",
    "\n",
    "Evaluated on our whole training set, the MSE loss function is:\n",
    "$$J(θ) ={1\\over4}_{x∈X}(f^∗(x) − f(x; θ))^2. $$\n",
    "\n",
    "Now we have to define our model. If we choose a linear model like y = w<sup>T</sup>.x + b and use normal equations to solve this we will get w = 0, b = 0.5 which will output 0.5 everywhere which is not correct this happenned because the function is not linear. \n",
    "\n",
    "One way to solve this problem is to use a model that learns a diﬀerent feature space in which a linear model is able to represent the solution.\n",
    "\n",
    "Speciﬁcally, we will introduce a simple feedforward network with one hidden layer containing two hidden units. The output layer is still just a linear regression model, but now it is applied to h rather than to x. where h is output of hidden layer.\n",
    "<img src='dffn/xor.jpg'>\n",
    "\n",
    "here w's are the weight's and θ's are the bias's of our model.\n",
    "\n",
    "We now have to define our activation functions for 3,4,5. In modern neural networks,the default recommendation is to use the <br>Rectiﬁed Linear Unit ( **ReLU(z) = max{0,z}** ).\n",
    "\n",
    "<br>Activation functions<br>\n",
    "A3 = ReLU(w3<sup>T</sup>.x + b3)<br>\n",
    "A4 = ReLU(w4<sup>T</sup>.x + b4)<br>\n",
    "A5 = w5<sup>T</sup>.h + b5\n",
    "\n",
    "Final form of our neural network.\n",
    "$$f(x; W , c, w, b) = w^T.max\\{0, W^Tx + c\\} + b$$\n",
    "\n",
    "We can then specify a solution to the XOR problem as:\n",
    "<br>\n",
    "```\n",
    "W = [1 1\n",
    "     1 1]\n",
    "     \n",
    "c = [0\n",
    "    -1]\n",
    "    \n",
    "w = [1\n",
    "    -2]\n",
    "    \n",
    "b = 0\n",
    "```\n",
    "\n",
    "> NOTE : we have not yet discussed how to obtain these parameter we only know that these parameters can solve the problem.\n",
    "\n",
    "Let's check it...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XOR example\n",
    "W = np.array([[1,1],[1,1]])\n",
    "c = np.array([[0],[-1]])\n",
    "w = np.array([[1],[-2]])\n",
    "b = 0\n",
    "\n",
    "X = np.array([[0, 0, 1, 1],[0, 1, 0, 1]])\n",
    "Y = np.array([0, 1, 1, 0])\n",
    "\n",
    "h = np.dot(W.T,X) + c # output of hiddrn units\n",
    "h = h*(h>0) # relu applied\n",
    "output = np.squeeze(np.dot(w.T,h) + b) # final output\n",
    "print(\"The output obtained is :\\n\",output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "### 2. Gradient-Based Learning\n",
    "\n",
    "Designing and training a neural network is not much diﬀerent from training anyother machine learning model with gradient descent.\n",
    "\n",
    "The major difference between the nueral nets and other learning algorithms is that non-linearity of neural nets causes cost function to become non-convex.\n",
    "\n",
    "#### 2.1 Cost Function\n",
    "\n",
    "An important aspect of the design of a deep neural network is the choice of the cost function. In most cases, our parametric model deﬁnes a distribution p(y|x,θ) andwe simply use the principle of maximum likelihood.\n",
    "\n",
    "i.e: we can use the cross entropy between the model prediction and the desired output as the cost fucntion.\n",
    "\n",
    "##### 2.1.1 Learning Conditional Distributions with Maximum Likelihood\n",
    "\n",
    "Most modern neural networks are trained using maximum likelihood. This means that the cost function is simply the negative log-likelihood:\n",
    "\n",
    "$$J(θ) = −E_{x,y∼p_{data}}\\log p_{model}(y\\ | \\ x).$$\n",
    "\n",
    "if model is gaussian model( *N(y;f(x;θ),I)*), then we recover the mean squared error cost,\n",
    "\n",
    "$$J(θ) ={1\\over2}E_{x,y∼p_{data}}\\|y − f(x; θ)\\|^2+ const$$\n",
    "\n",
    "One objective of defining cost fucntion for gradient descent algorithms is that the gradient should be large enough for guiding the learning model towards the solution and so should avoid the use of functions which gets saturated or flat as much as possible.\n",
    "\n",
    "##### 2.1.2 Learning Conditional Statistics\n",
    "\n",
    "Instead of learning a full probability distribution *p(y | x;θ)*, we often want to learn just one conditional statistic of y given x.\n",
    "> Example, we may have f(x;θ) to predict the mean of y.\n",
    "\n",
    "We want to design a powerful nueral net that allows f to become any function from a wide class, hence we can see the cost fucntion as a functional (A functional is a mapping from functions to real numbers.) rather than a function. Thus we can solve our problme w.r.t function instead of parameters.\n",
    "\n",
    "Solving an optimization problem with respect to a function requires a mathematical tool called ***calculus of variations***, which give these two powerful results:\n",
    "\n",
    "$$f∗= arg\\ min_f\\ E_{x,y∼p_{data}}\\|y − f(x)\\|^2$$f predicts the mean of y for each x.\n",
    "<br>and,\n",
    "$$f∗= arg\\ min_f\\ E_{x,y∼p_{data}}\\|y − f(x)\\|$$f predicts the median of y for each x.\n",
    "\n",
    "#### 2.2 Output Units\n",
    "\n",
    "The cost function more typically the cross entropy will be based on the output we take from the nueral network.\n",
    "\n",
    "##### 2.2.1 Linear Units for Gaussian Output Distributions\n",
    "\n",
    "One simple kind of output unit is based on an aﬃne transformation with no nonlinearity. These are often just called linear units.\n",
    "\n",
    "$$y=W^T.h+b$$\n",
    "\n",
    "which are mean of a guassian distribution,so maximizing the log-likelihood is then equivalent to minimizing the mean squared error.\n",
    "\n",
    "##### 2.2.2 Sigmoid Units for Bernoulli Output Distributions\n",
    "\n",
    "A sigmoid output unit is deﬁned by:\n",
    "\n",
    "$$y = σ\\left(wT.h + b\\right)$$\n",
    "\n",
    "where z = w.h +b is called a **logit** defining the exponent over which the exponential distribution is based.\n",
    "This approach to predicting the probabilities in log space is natural to use with maximum likelihood learning.\n",
    "\n",
    "When we use other loss functions, such as mean squared error, the loss can saturate anytime σ(z) saturates. The sigmoid activation function saturates to 0 when z becomes very negative and saturates to 1 when z becomes very positive. For this reason, maximum likelihood is almost always the preferred approach to training sigmoid output units which has a counter effect to exponential term due to log.\n",
    "\n",
    "##### 2.2.3 Softmax Units for Multinoulli Output Distributions\n",
    "\n",
    "Any time we wish to represent a probability distribution over a discrete variable with n possible values, we may use the softmax function. Softmax functions are most often used as the output of a n-dimensional classiﬁer:\n",
    "\n",
    "$$softmax(z)_i={exp(z_i)\\over\\sum_jexp(z_j)}$$\n",
    "\n",
    "as you would have noticed this a more general form than sigmoid and here also maximum likelihood is almost always the preferred approach to train.\n",
    "\n",
    "\n",
    "##### 2.2.4 Other Output Types\n",
    "\n",
    "The linear, sigmoid, and softmax output units described above are the most common. Neural networks can generalize to almost any kind of output layer that we wish. The principle of maximum likelihood provides a guide for how to design a good cost function for nearly any kind of output layer.\n",
    "\n",
    "> For great learning insights, and a lot of exposure to posibilities you should study see pages 182-183, 185-187 in detail here: http://www.deeplearningbook.org/contents/mlp.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax vizulaization and maximum likelihood linearity:\n",
    "def softmax(z):\n",
    "    n = np.exp(z)\n",
    "    d = np.sum(n)\n",
    "    return(n/d)\n",
    "    \n",
    "n = 100\n",
    "x = np.linspace(-2,2,n)\n",
    "y = softmax(x)\n",
    "print(\"The sum of all values of softmax(x) = \",np.sum(y))\n",
    "\n",
    "f, ax= plt.subplots(1,2,figsize=[12,4])\n",
    "ax[0].plot(x,y,label=\"softmax\")\n",
    "yd = np.log(y)/100\n",
    "ax[1].plot(x,yd,label=\"log(softmax)\")\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------\n",
    "\n",
    "### 3. Hidden units\n",
    "\n",
    "The design of hidden units is an e xtremely active area of research and does not yet have many deﬁnitive guiding theoretical principles. Rectiﬁed linear units are an excellent default choice of hidden units.\n",
    "\n",
    "Although some of the activations like ReLU are not differentiable at all points gradient descent seems to work on them since the nueral network does not optimize the funtion to arrive at local minima but to get close to it as much as possible also we can define a gradient e.g: for ReLU gradient at 0 to be same as that of 0+ or 0- points. We can approximate so since, When a function is asked to evaluate g(0), it is very unlikely that the underlying value truly was 0. Instead, it was likely to be some small value that was rounded to 0.\n",
    "\n",
    "#### 3.1 Rectiﬁed Linear Units and Their Generalizations\n",
    "Easy to optimize due to linearity :<br> ***ReLU***\n",
    "\n",
    "\n",
    "$$ g(z) = max\\{0, z\\}$$\n",
    "\n",
    "One drawback to rectiﬁed linear units is that they cannot learn via gradient-based methods on examples for which their activation is zero.\n",
    "\n",
    "Three generalizations of rectiﬁed linear :\n",
    "\n",
    "***Leaky ReLU***:\n",
    "\n",
    "$$g(z, α)_i=max(0, z_i) +α_imin(0, z_i)$$\n",
    "\n",
    "α is a small value like 0.01\n",
    "\n",
    "***Parametric ReLU***\n",
    "\n",
    "Tries to learn α and cosider it as a parameter.\n",
    "\n",
    "***Maxout units*** \n",
    "\n",
    "generalize rectiﬁed linear units further. Instead of applying an element-wise functiong(z), maxout units divide z into groups of k values. Each maxout unit then outputs the maximum element of one of these groups:\n",
    "\n",
    "$$g(z)i= max_{j∈G_{(i)}}z_j$$\n",
    "\n",
    "This provides a way of learning a piecewise linear function that responds to multiple directions in the input x space. A maxout unit can learn a piecewise linear, convex function with up to k pieces. Maxout units can thus be seen as learning the activation function itself rather than just the relationship between units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Logistic Sigmoid and Hyperbolic Tangent\n",
    "\n",
    "Prior to the introduction of rectiﬁed linear units, most neural networks used the logistic sigmoid activation function:\n",
    "\n",
    "$$g(z) = σ(z)$$\n",
    "\n",
    "or, the hyperbolic tangent activation function:\n",
    "\n",
    "$$g(z) = tanh(z)$$\n",
    "\n",
    "The sigmoid fucntions are now more typically used for output layer and not in hidden layers since its learning curve saturates far from 0. tanh is very similar to sigmoid function but has advantage over sigmoid since training a deep neural network *y=w.tanh(U.tanh(V.x))* resembles training a linear model *y = w.U.V.x* as long as the activations are kept small also tanh has negative ranged values for output which sigmoid does not give directly.\n",
    "\n",
    "#### 3.3 Other Hidden Units\n",
    "\n",
    "Many other types of hidden units are possible but are used less frequently. Like softmax can be used to design network with switches and a few resonable ones are:\n",
    "\n",
    "***RBF***:\n",
    "\n",
    "This function becomes more active as x approaches a template W<sub>:,i</sub>. Because it saturates to 0 for most x, it can be diﬃcult to optimize...\n",
    "$$h_i=exp\\left(−{1\\overσ^2_i}\\|W_{:,i}− x\\|2\\right)$$\n",
    "\n",
    "***Softplus***:\n",
    "\n",
    "This is a smooth version of the rectiﬁer...\n",
    "$$g(a) =ζ(a) =log(1+e^a)$$\n",
    "\n",
    "The softplus demonstrates that the performance of hidden unit types can be very counterintuitive one might expect it to have an advantage over the rectiﬁer due to being diﬀerentiable everywhere or due to saturating less completely, but empirically it does not\n",
    "\n",
    "\n",
    "***Hard tanh***:\n",
    "\n",
    "This is shaped similarly to the tanh and the rectiﬁer, but unlike the latter, it is bounded :\n",
    "\n",
    "$$g(a) = max(−1, min(1, a))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualization of activation functions:\n",
    "def relu(z):\n",
    "    o = np.maximum(z,0)\n",
    "    return o\n",
    "\n",
    "def leakyrelu(z,alpha):\n",
    "    o = np.maximum(z,alpha*z)\n",
    "    return o\n",
    "    \n",
    "def rbf(z,sigma,mu):\n",
    "    o = (-1/sigma**2)*(z-mu)**2\n",
    "    return o\n",
    "\n",
    "def softplus(z):\n",
    "    o = np.log(1+np.exp(z))\n",
    "    return o\n",
    "\n",
    "def sigmoid(z):\n",
    "    o = 1/(1+np.exp(-z))\n",
    "    return o\n",
    "    \n",
    "def tanh(z):\n",
    "    o = np.tanh(z)\n",
    "    return o\n",
    "\n",
    "def hardtanh(z):\n",
    "    o = np.maximum(-1,np.minimum(z,1))\n",
    "    return o\n",
    "\n",
    "n = 50\n",
    "x = np.linspace(-10,10,n)\n",
    "rx = relu(x)\n",
    "rl = leakyrelu(x,0.1)\n",
    "rr = rbf(x,1,0)\n",
    "rp = softplus(x)\n",
    "rs = sigmoid(x)\n",
    "rt = tanh(x)\n",
    "rh = hardtanh(x)\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(2,4,figsize=[16,8])\n",
    "ax[0,0].plot(x,x)\n",
    "ax[0,0].set_title(\"Linear\")\n",
    "ax[0,1].plot(x,rx)\n",
    "ax[0,1].set_title(\"ReLU\")\n",
    "ax[0,2].plot(x,rl)\n",
    "ax[0,2].set_title(\"Leaky ReLU\")\n",
    "ax[0,3].plot(x,rr)\n",
    "ax[0,3].set_title(\"RBF\")\n",
    "ax[1,0].plot(x,rp)\n",
    "ax[1,0].set_title(\"Softplus\")\n",
    "ax[1,1].plot(x,rs)\n",
    "ax[1,1].set_title(\"Sigmoid\")\n",
    "ax[1,2].plot(x,rt)\n",
    "ax[1,2].set_title(\"tanh\")\n",
    "ax[1,3].plot(x,rh)\n",
    "ax[1,3].set_title(\"Hard tanh\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Architecture Design\n",
    "\n",
    "The word architecture refers to the overall structure of the network: how many units it should have and how these units should be connected to each other.\n",
    "\n",
    "#### 4.1 Universal Approximation Properties and Depth\n",
    "\n",
    "There are two reasons for whichi a nueral network could fail:\n",
    "\n",
    "   - First, the optimization algorithm used for training may not be able to ﬁnd the value of the parameters that corresponds to the desired function.\n",
    "   - Second, the training algorithm might choose the wrong function as a result of overﬁtting.\n",
    "   \n",
    "According to the ***universal approximation theorem***, there exists a network large enough to achieve any degree of accuracy we desire, but the theorem does not say how large this network will be.\n",
    "\n",
    "A feedforward network with a single layer is suﬃcient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly. In many circumstances, using deeper models can reduce the number of units required to represent the desired function and can reduce the amount of generalization error. But having a model with more than required layers may lead to overfitting.\n",
    "\n",
    "> See fig 6.5 (pg 196) here : http://www.deeplearningbook.org/contents/mlp.html read upto page 199 for good insights.\n",
    "\n",
    "-----------------\n",
    "\n",
    "### 5. Back-Propagation and Other Diﬀerentiation Algorithms\n",
    "\n",
    "we have propagated the information from input layer to ouput layer and computed the cost function. This is called ***forward propagation*** but as in gradient descent we need to compute gradient of cost function w.r.t each paramater so we pass the gradient information from output layer back to input layer. This is called the ***back propagation*** (backprop). After which we can use batch bradient descent or stochastic gradient descent to optimize the parameters.\n",
    "\n",
    "#### 5.1 Computational Graphs\n",
    "\n",
    "we can see each node in network as a variable indication with set of allowable operations and then we can turn our nework into a graphical structure to compute in a graphical manner.\n",
    "\n",
    "#### 5.2 Chain Rule of Calculus\n",
    "\n",
    "During backpropagation we can compute the differentiation of cost function w.r.t to given parameter using chain rule of derivatives and we need not calculate the derivaive analytically for every parameter.\n",
    "\n",
    "Suppose that x ∈ R<sup>m</sup>, y ∈ R<sup>n</sup>,g maps from R<sup>m</sup> to R<sup>n</sup>, and f maps from R<sup>n</sup> to R. If y=g(x) and z=f(y), then:\n",
    "\n",
    "$${∂z\\over∂x_i}=\\sum_j{∂z\\over∂y_j}.{∂y_j\\over∂x_i}$$\n",
    "\n",
    "#### 5.3 Recursively Applying the Chain Rule to Obtain Backprop\n",
    "\n",
    "We start moving from output layer computing the gradient of cost with respect to parameters and apply the chain rule to the gradient obtained from (i+1)th layer to (i)th layer to calculate the gradient w.r.t (i)th layer parameters. Activation functions and input vector of each node in our computation graphs define how each of these gradient will turn out to be.\n",
    "\n",
    "**Forward propagation Algorithm (6.3) :**\n",
    "> Read here page 208 : http://www.deeplearningbook.org/contents/mlp.html\n",
    "\n",
    "**Back propagation Algorithm (6.4) :**\n",
    "> Read here page 209 : http://www.deeplearningbook.org/contents/mlp.html\n",
    "\n",
    "For an hand computation exmaple of back propagation--\n",
    "> Example back propagation: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "\n",
    "You would have noticed by now that the learning gradient decreases from output to input layer resulting in slow learning for near input layers, this is the reason we usually go fo ReLU as compared to Sigmoid activation functions.\n",
    "\n",
    "Let's implement the back progation for the XOR example we were doing...\n",
    "<br>\n",
    "we will maintain linear and activation caches of inputs and gradients while computing to make things easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling the data\n",
    "X = np.array([[0, 1, 0, 1],[0, 0, 1, 1]]) # input data structure 2x4\n",
    "Y = np.array([[0, 1, 1, 0]]) # output data structure 1x4\n",
    "\n",
    "layers = [2,2,1] # architecture of our network 2(input) --> 2(hidden) --> 1(output)\n",
    "\n",
    "print(\"Representing data\")\n",
    "plt.scatter(X[0,:].T,X[1,:].T,c=Y.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's define the activation functions and there gradients with respect to input next layer gradient: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining activation functions and gradients w.r.t input:\n",
    "def relu(z): # in actual its a leaky relu performs better\n",
    "    o = z*(z>0) - 0.1*(z<0)*z\n",
    "    return o, z\n",
    "\n",
    "def drelu(dA,z):# in actual its a diff leaky relu performs better\n",
    "    o = (z>0)*1 - 0.1*(z<0)\n",
    "    return o*dA\n",
    "\n",
    "def sigmoid(z):\n",
    "    o = 1/(1+np.exp(-z))\n",
    "    return o, z\n",
    "\n",
    "def dsigmoid(dA,z):\n",
    "    o = 1/(1+np.exp(-z))\n",
    "    return (o*(1-o))*dA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Initialize out parameters for different layers..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialization of parameters w's and b's\n",
    "def initparams(layers):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layers     -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(15)\n",
    "    parameters = {}\n",
    "    L = len(layers) # length/ depth of the network\n",
    "    \n",
    "    # NOTE : Representing w's and b's as matrices having dimensions, rows -> (i)th layer size, columns -> (i-1)th layer size:\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers[l], layers[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layers[l], 1))\n",
    "        assert(parameters['W' + str(l)].shape == (layers[l], layers[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layers[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the linear part of a layer's forward propagation z = W.h + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linfwd(A, W, b):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W,A)+b\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the forward propagation for the LINEAR->ACTIVATION layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linactfwd(A_prev,W,b,act):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if act == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linfwd(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif act == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linfwd(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)  # cache = ((input, W, b), (output, z)) for that layer\n",
    "    return A, cache    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define forward propagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lmodelfwd(X, parameters):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2     # number of layers in the neural network L = len(w's) + len(b's) = 2xdepth\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linactfwd(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],\"relu\")\n",
    "        caches.append(cache) \n",
    "        \n",
    "    AL, cache = linactfwd(A,parameters['W' + str(L)],parameters['b' + str(L)],\"sigmoid\")\n",
    "    \n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the cost fucntion: ** cross entropy cost **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compcost(AL, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector , shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # training batch size\n",
    "    # cross entropy cost\n",
    "    \n",
    "    cost = (-1/m)*(np.dot(Y,np.log(AL).T)+np.dot(1-Y,np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the linear portion of backward propagation for a single layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linback(dZ, cache):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the backward propagation for the LINEAR->ACTIVATION layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linactback(dA, cache, act):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if act == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = drelu(dA, activation_cache)\n",
    "        dA_prev, dW, db = linback(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif act == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = dsigmoid(dA, activation_cache)\n",
    "        dA_prev, dW, db = linback(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Implement the backward propagation completely:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lmodelback(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector \n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    en = 10**-20 # a very small value (epsilon)\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    n = AL.shape[0]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    \n",
    "    \n",
    "    try: \n",
    "        dAL = (-np.divide(Y, AL) + np.divide(1 - Y, 1 - AL))/m\n",
    "    except ZeroDivisionError: \n",
    "        dAL = (-np.divide(Y, AL+np.sign(AL)*en) + np.divide(1 - Y, 1 - AL + np.sign(1-AL)*en))/m\n",
    "        \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linactback(dAL, current_cache, \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)]\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linactback(grads[\"dA\" + str(l+2)], current_cache, \"relu\")\n",
    "        \n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great now define a function to update parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateparams(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def llayermodel(X, Y, layers, learning_rate = 0.0007, num_epochs = 10000, print_cost = True, printerval = 1000):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initparams(layers)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        a3, caches = lmodelfwd(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compcost(a3, Y)\n",
    "        \n",
    "        # Backward propagation\n",
    "        grads = lmodelback(a3, Y, caches)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = updateparams(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % printerval == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if i % printerval == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations /'+str(printerval))\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prediction function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X,paramerters):\n",
    "    AL, caches = lmodelfwd(X, parameters)\n",
    "    return AL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all the requirements have been met now we can train our model, so let's jump to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = llayermodel(X, Y, layers, learning_rate = 1, num_epochs = 10000, print_cost = True, printerval = 500)\n",
    "\n",
    "# prediction....\n",
    "yd = predict(X,parameters)\n",
    "print(parameters)\n",
    "print(\"The Predcited values are\\n\",yd,\"\\nalmost equal to the output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Historical Notes\n",
    "\n",
    "> Read from reference : http://www.deeplearningbook.org/contents/mlp.html\n",
    "\n",
    "------------------------\n",
    "\n",
    "**NOTE** :* From next chapter this tutorial will only emphasise on the coding or examples part the theory will have to be studied from the reference book given...Reference pages will be provided if necessary.*\n",
    "\n",
    "\n",
    "## Congratulation\n",
    "on completing the deep nueral network part next we will introduce regularization in our models..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

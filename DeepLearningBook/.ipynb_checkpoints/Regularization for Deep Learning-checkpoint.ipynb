{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization for Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familier with following concepts you can move to next notebook:\n",
    "> Reference http://www.deeplearningbook.org/contents/regularization.html\n",
    "\n",
    "    1. Parameter Norm Penalties\n",
    "    2. Norm Penalties as Constrained Optimization\n",
    "    3. Regularization and Under-Constrained Problems\n",
    "    4. Dataset Augmentation\n",
    "    5. Noise Robustness\n",
    "    6. Semi-Supervised Learning\n",
    "    7. Multitask Learning\n",
    "    8. Early Stopping\n",
    "    9. Parameter Tying and Parameter Sharing\n",
    "    10. Sparse Representations\n",
    "    11. Bagging and Other Ensemble Methods\n",
    "    12. Dropout\n",
    "    13. Adversarial Training\n",
    "    14. Tangent Distance, Tangent Prop and ManifoldTangent Classiﬁer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0, './reg')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import oneoutnn as nn # the nueral network model we created in previous lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A central problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are known collectively as regularization.\n",
    "\n",
    "> Read page 224 - 225 : http://www.deeplearningbook.org/contents/regularization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parameter Norm Penalties\n",
    "\n",
    "> Read page 226 - 233: L2, L1 regularizations and there effects and usefull ness....\n",
    "\n",
    "to get a better understanding of implementation of L1 regularization read here : http://ufldl.stanford.edu/wiki/index.php/Sparse_Coding:_Autoencoder_Interpretation\n",
    "\n",
    "also for applications of these methods like lasso and ridge using l2 and l1 respectively read here :<br>\n",
    "https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/\n",
    "\n",
    "Let's apply L2 regularization to our model....\n",
    "we only need to change cost function and update rules...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the data to fit.......\n",
    "np.random.seed(1)\n",
    "n_points = 20\n",
    "# we are limiting the output to 0 to one since the output node gives probability between 0 to 1\n",
    "X = np.linspace(0,0.8,n_points).reshape(n_points,1)\n",
    "np.random.shuffle(X)\n",
    "X = X.T\n",
    "Y = abs(X**2)+np.random.randn(1,n_points)*0.01  + 0.3\n",
    "Y = 0.8*Y/np.max(Y)\n",
    "print(\"We will try to fit a quadratic data\")\n",
    "plt.plot(X.T,Y.T,'.')\n",
    "plt.show()\n",
    "\n",
    "tr_size = 16\n",
    "x_tr = X[0,:tr_size].reshape(1,tr_size)\n",
    "y_tr = Y[0,:tr_size].reshape(1,tr_size)\n",
    "x_te = X[0,tr_size:].reshape(1,n_points-tr_size)\n",
    "y_te = Y[0,tr_size:].reshape(1,n_points-tr_size)\n",
    "print(\"the training data...\")\n",
    "plt.plot(x_tr.T,y_tr.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets train using the model we created in previous lecture and see the performance\n",
    "layers = [1,15,1]\n",
    "params = nn.llayermodel(x_tr, y_tr, layers, learning_rate=0.9, num_epochs=150000, print_cost=True, printerval=10000)\n",
    "pY = nn.predict(x_te,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining new cost function\n",
    "def compcost_reg(AL, Y, ld, parameters, reg):\n",
    "    m = Y.shape[1]  # training batch size\n",
    "    # cross entropy cost\n",
    "\n",
    "    cost = (-1 / m) * (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    if (reg=='l2reg'):\n",
    "        for l in range(L):\n",
    "            cost = cost + ld*np.sum(np.sum(parameters[\"W\" + str(l + 1)]**2))/2/m\n",
    "    \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining new update rules:\n",
    "def updateparams_reg(parameters, grads, learning_rate, ldbym, reg):\n",
    "    \n",
    "    \n",
    "    \n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        if (reg=='l2reg'):\n",
    "            parameters[\"W\" + str(l + 1)] = (1-ldbym)*parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        else:\n",
    "            parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "            \n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)] \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining new model with regularization parameter:........\n",
    "def llayermodel(X, Y, layers, learning_rate=0.0007, ld = 0.001, reg = 'l2reg', num_epochs=10000, print_cost=True, printerval=1000):\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "    \n",
    "    [n, m] = X.shape\n",
    "    # Parameters initialization.\n",
    "    parameters = nn.initparams(layers)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    if (reg == 'l2reg'):\n",
    "        for i in range(0, num_epochs):\n",
    "            # Forward propagation\n",
    "            a3, caches = nn.lmodelfwd(X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = compcost_reg(a3, Y, ld, parameters, reg)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = nn.lmodelback(a3, Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            parameters = updateparams_reg(parameters, grads, learning_rate, ld/m, reg)\n",
    "\n",
    "            # Print the cost every 100 training example\n",
    "            if print_cost and i % printerval == 0:\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            if i % printerval == 0:\n",
    "                costs.append(cost)\n",
    "    else:\n",
    "        for i in range(0, num_epochs):\n",
    "            # Forward propagation\n",
    "            a3, caches = nn.lmodelfwd(X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = nn.compcost(a3, Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = nn.lmodelback(a3, Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            parameters = nn.updateparams(parameters, grads, learning_rate)\n",
    "\n",
    "            # Print the cost every 100 training example\n",
    "            if print_cost and i % printerval == 0:\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            if i % printerval == 0:\n",
    "                costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations /' + str(printerval))\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets train now with our new model.............\n",
    "layers = [1,15,1]\n",
    "params = llayermodel(x_tr, y_tr, layers, learning_rate=0.9, ld = 0.0002, reg='l2reg', num_epochs=150000, print_cost=True, printerval=10000)\n",
    "pY = nn.predict(x_te,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> you will see that the MSE of regularized model is less than the non-regularized since the overtraining complex model like this have increased the overfitting and hence the generalization gap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Norm Penalties as Constrained Optimization\n",
    "we can use norm penalties as a contrain condition over the solution we want to get see how:\n",
    "\n",
    "> Read section 7.2 on reference book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Regularization and Under-Constrained Problems\n",
    "In some cases, regularization is necessary for machine learning problems to be properly deﬁned.\n",
    "\n",
    "> Read section 7.3 on reference book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset Augmentation\n",
    "\n",
    "To generalize more better we augment the data we have to get more robust model see how:\n",
    "\n",
    "> Read section 7.4 on reference book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Noise Robustness\n",
    "\n",
    "Noise can be introduced in hidden units, parameters or in output units of the network to make it robust see a brief about these:\n",
    "\n",
    "> Read Section 7.5 on reference book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Semi Supervised Learning\n",
    "\n",
    "In the paradigm of semi-supervised learning, both unlabeled examples from P(x) and labeled examples from P(x, y) are used to estimate P(y | x) or predict y from x. The goal is to learn a representation so that examples from the same class have similar representations. Examples that cluster tightly in the input space should be mapped to similar representations.\n",
    "\n",
    "> Read section 7.6\n",
    "\n",
    "### 7. Multitask Learning\n",
    "\n",
    "> Read Section 7.7\n",
    "\n",
    "### 8. Early Stopping\n",
    "\n",
    "Early stopping means storing the or stopping the training when we reach minimum validation set error to prevent overfitting and return the parameters wuth minimum generalization gap.\n",
    "\n",
    ">Read Section 7.8\n",
    "\n",
    "Let's implement this in our model......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining data\n",
    "str_size = 12\n",
    "x_str = x_tr[0,:str_size].reshape(1,str_size)\n",
    "y_str = y_tr[0,:str_size].reshape(1,str_size)\n",
    "x_val = x_tr[0,str_size:].reshape(1,tr_size-str_size)\n",
    "y_val = y_tr[0,str_size:].reshape(1,tr_size-str_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining new model with regularization parameter:........\n",
    "def llayermodel(X, Y, xval, yval, layers, learning_rate = 0.0007, ld = 0.001, \n",
    "                reg = 'none', estop = True, max_passes = 100, num_epochs=10000, print_cost=True, printerval=1000):\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "    \n",
    "    [n, m] = X.shape\n",
    "    # Parameters initialization.\n",
    "    parameters = nn.initparams(layers)\n",
    "\n",
    "    valcst = 10**5\n",
    "    passes = 0\n",
    "    oparams = parameters\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    if (reg == 'l2reg'):\n",
    "        for i in range(0, num_epochs):\n",
    "            # Forward propagation\n",
    "            a3, caches = nn.lmodelfwd(X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = compcost_reg(a3, Y, ld, parameters, reg)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = nn.lmodelback(a3, Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            parameters = updateparams_reg(parameters, grads, learning_rate, ld/m, reg)\n",
    "\n",
    "            # Print the cost every 100 training example\n",
    "            if print_cost and i % printerval == 0:\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            if i % printerval == 0:\n",
    "                costs.append(cost)\n",
    "                \n",
    "            if(estop and i%10 == 0):    \n",
    "                yvald = nn.predict(xval,parameters)\n",
    "                vcost = compcost_reg(yvald, yval, ld, parameters, reg)\n",
    "                if(vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = parameters\n",
    "                else:\n",
    "                    if(passes > max_passes):\n",
    "                        parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "    else:\n",
    "        for i in range(0, num_epochs):\n",
    "            # Forward propagation\n",
    "            a3, caches = nn.lmodelfwd(X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            cost = nn.compcost(a3, Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = nn.lmodelback(a3, Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            parameters = nn.updateparams(parameters, grads, learning_rate)\n",
    "\n",
    "            # Print the cost every 100 training example\n",
    "            if print_cost and i % printerval == 0:\n",
    "                print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "            if i % printerval == 0:\n",
    "                costs.append(cost)\n",
    "                \n",
    "            if(estop and i%10 == 0):    \n",
    "                yvald = nn.predict(xval,parameters)\n",
    "                vcost = nn.compcost(yvald, yval)\n",
    "                if(vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = parameters\n",
    "                else:\n",
    "                    if(passes > max_passes):\n",
    "                        parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations /' + str(printerval))\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]\n",
    "params = llayermodel(x_str, y_str, x_val, y_val, layers, learning_rate = 0.9, ld = 0.001, reg = 'none', estop = True, max_passes = 100, num_epochs=150000, print_cost=True, printerval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = nn.predict(x_te,params)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We got the better result with out iterating for 150000 iterations which is quiet remarkable if you think it saves the computaion time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---------------\n",
    "\n",
    "### 9. Parameter Tying and Parameter Sharing\n",
    "\n",
    "This part will be related to convolutional nueral network understsanding:.... see how and why how parameter sharing reduces the memory footprint of the model required to train multiple classifiers.\n",
    "\n",
    "> Read section 7.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Sparse Representations\n",
    "There are many ways to regularize a model to make it sparse. one way we discussed already was L1 regularization in which the weights were constrained to become zero in some places. Similarily we can make outputs of hidden nodes to be contrained to become zero.\n",
    "\n",
    "> Read section 7.10\n",
    "\n",
    "Let's see what weights do we get with L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining new cost function\n",
    "def compcost_reg(AL, Y, ld, parameters, reg):\n",
    "    m = Y.shape[1]  # training batch size\n",
    "    # cross entropy cost\n",
    "\n",
    "    cost = (-1 / m) * (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T))\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    if (reg == 'l2reg'):\n",
    "        for l in range(L):\n",
    "            cost = cost + ld*np.sum(np.sum(parameters[\"W\" + str(l + 1)]**2))/2/m\n",
    "    \n",
    "    elif(reg == 'l1reg'):\n",
    "        alpha = 10**-20\n",
    "        for l in range(L):\n",
    "            cost = cost + ld*np.sum(np.sum(np.sqrt(parameters[\"W\" + str(l + 1)]**2 + alpha)))/m    \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining new update rules:\n",
    "def updateparams_reg(parameters, grads, learning_rate, ldbym, reg):\n",
    "    \n",
    "    \n",
    "    \n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        if (reg == 'l2reg'):\n",
    "            parameters[\"W\" + str(l + 1)] = (1-ldbym)*parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        elif(reg == 'l1reg'):\n",
    "            parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)] - ldbym*np.sign(parameters[\"W\" + str(l + 1)])\n",
    "        else:\n",
    "            parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "            \n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * grads[\"db\" + str(l + 1)] \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining new model with regularization parameter:........\n",
    "def llayermodel_2(X, Y, xval, yval, layers, learning_rate = 0.0007, ld = 0.001, \n",
    "                reg = 'none', estop = True, max_passes = 100, num_epochs=10000, print_cost=True, printerval=1000, plotcost = True):\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "    \n",
    "    [n, m] = X.shape\n",
    "    # Parameters initialization.\n",
    "    parameters = nn.initparams(layers)\n",
    "\n",
    "    valcst = 10**5\n",
    "    passes = 0\n",
    "    oparams = parameters\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    if (reg == 'l2reg' or reg == 'l1reg'):\n",
    "        for i in range(0, num_epochs):\n",
    "            # Forward propagation\n",
    "            a3, caches = nn.lmodelfwd(X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            if(plotcost):\n",
    "                cost = compcost_reg(a3, Y, ld, parameters, reg)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = nn.lmodelback(a3, Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            parameters = updateparams_reg(parameters, grads, learning_rate, ld/m, reg)\n",
    "\n",
    "            # Print the cost every 100 training example\n",
    "            if(plotcost):\n",
    "                if print_cost and i % printerval == 0:\n",
    "                    print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                if i % printerval == 0:\n",
    "                    costs.append(cost)\n",
    "                \n",
    "            if(estop and i%10 == 0):    \n",
    "                yvald = nn.predict(xval,parameters)\n",
    "                vcost = compcost_reg(yvald, yval, ld, parameters, reg)\n",
    "                if(vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = parameters\n",
    "                else:\n",
    "                    if(passes > max_passes):\n",
    "                        parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "                    \n",
    "                    \n",
    "                \n",
    "            \n",
    "    else:\n",
    "        for i in range(0, num_epochs):\n",
    "            # Forward propagation\n",
    "            a3, caches = nn.lmodelfwd(X, parameters)\n",
    "\n",
    "            # Compute cost\n",
    "            if(plotcost):\n",
    "                cost = nn.compcost(a3, Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = nn.lmodelback(a3, Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            parameters = nn.updateparams(parameters, grads, learning_rate)\n",
    "\n",
    "            # Print the cost every 100 training example\n",
    "            if(plotcost):\n",
    "                if print_cost and i % printerval == 0:\n",
    "                    print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                if i % printerval == 0:\n",
    "                    costs.append(cost)\n",
    "                \n",
    "            if(estop and i%10 == 0):    \n",
    "                yvald = nn.predict(xval,parameters)\n",
    "                vcost = nn.compcost(yvald, yval)\n",
    "                if(vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = parameters\n",
    "                else:\n",
    "                    if(passes > max_passes):\n",
    "                        parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "    # plot the cost\n",
    "    if(plotcost):\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations /' + str(printerval))\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]\n",
    "params = llayermodel_2(x_str, y_str, x_val, y_val, layers, learning_rate = 0.9, ld = 0.001, reg = 'l1reg', estop = False, max_passes = 100, num_epochs=150000, print_cost=True, printerval=10000, plotcost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = nn.predict(x_te,params)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params)\n",
    "print(\"\\nAs you can see many of the weight parameters are nearly zero...\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Bagging and Other Ensemble Methods\n",
    "Every model involves some error for a given data samples but it's not necessary that they make related errors and so one can compensate for another so averaging over these models can be done which is called bagging in which we make k models with k datasets which are obtained from original dataset alterations.\n",
    "\n",
    "> Read section 7.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining bags\n",
    "def make_bags(x,y,k):\n",
    "    m = y.shape[1]\n",
    "    np.random.seed(12)\n",
    "    bags = {}\n",
    "    for i in range(k):\n",
    "        permutation = np.random.randint(0,m-1,m) + (np.random.randn(1,m)>0)\n",
    "        bags['bag_x' + str(i)] = x[:,permutation]\n",
    "        bags['bag_y' + str(i)] = y[:,permutation]\n",
    "        \n",
    "    return bags\n",
    "\n",
    "def train_bagging(x_str, y_str, x_val, y_val, layers, k = 5, learning_rate = 0.9, ld = 0.001, reg = 'l1reg', estop = False, max_passes = 100, num_epochs=15000, print_cost=False, printerval=1000):\n",
    "    \n",
    "    bags = make_bags(x_str,y_str,k)\n",
    "    params = {}\n",
    "    for i in range(k):\n",
    "        print('Training bag no.' + str(i))\n",
    "        params['p'+ str(i)] = llayermodel_2(x_str, y_str, x_val, y_val, layers, learning_rate = learning_rate, ld = ld, \n",
    "                                          reg = reg, estop = estop, max_passes = max_passes, num_epochs=num_epochs, print_cost=print_cost, printerval=printerval, plotcost=False)\n",
    "    \n",
    "    return params\n",
    "\n",
    "def predict_bagging(x,params):\n",
    "    k = len(params)\n",
    "    pY = nn.predict(x,params['p0'])\n",
    "    for i in range(1,k):\n",
    "        pY = pY + nn.predict(x,params['p' + str(i)])\n",
    "        \n",
    "    pY = pY/k\n",
    "    \n",
    "    return pY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsbagging = train_bagging(x_str, y_str, x_val, y_val, layers, k = 10, learning_rate = 0.9, ld = 0.001, reg = 'none', estop = True, max_passes = 50, num_epochs=150000, print_cost=False, printerval=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = predict_bagging(x_te,paramsbagging)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Dropout\n",
    "\n",
    "Similar to bagging which create different models dropout trains the same model over different minibatches and simultaneously eliminating random nodes on each level to create different network structure each time which indirectly removes the contribution of uncessary nodes using random masks on the layer. So far, dropout remains the most widely used implicit ensemble method.\n",
    "\n",
    "> Read Section 7.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for implementation of dropout we need to modify the layer calculation rules:\n",
    "def lmodelfwd_reg(X, parameters, streg = 'none',stld = 0.5):\n",
    "\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "\n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network L = len(w's) + len(b's) = 2xdepth\n",
    "    masks = {}\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        if (streg == 'dout'):\n",
    "            A, cache = nn.linactfwd(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "            mask = np.random.binomial(1,stld,size=A.shape) / stld\n",
    "            masks['m'+str(l)] = mask\n",
    "        else:\n",
    "            A, cache = nn.linactfwd(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        \n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = nn.linactfwd(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmodelback_reg(AL, Y, caches, masks, streg= 'none'):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches)  # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    n = AL.shape[0]\n",
    "    Y = Y.reshape(AL.shape)  \n",
    "\t# after this line, Y is the same shape as AL\n",
    "    # Initializing the backpropagation\n",
    "\t# Zero divide avoided\n",
    "    en = 10**-20 # a very small number\n",
    "\n",
    "    try:\n",
    "        dAL = (-np.divide(Y, AL) + np.divide(1 - Y, 1 - AL))/m\n",
    "    except ZeroDivisionError:\n",
    "        dAL = (-np.divide(Y, AL+np.sign(AL)*en) + np.divide(1 - Y, 1 - AL + np.sign(1-AL)*en))/m\n",
    "\n",
    "    #dAL = -(1/m)*(Y-AL)\n",
    "\n",
    "\n",
    "    current_cache = caches[L - 1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = nn.linactback(dAL, current_cache, \"sigmoid\")\n",
    "    grads[\"dA\" + str(L)] = grads[\"dA\" + str(L)]*masks['m' + str(L-1)]\n",
    "    for l in reversed(range(L - 1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l]\n",
    "        if (streg=='dout' and l > 1):\n",
    "            dA_prev_temp, dW_temp, db_temp = nn.linactback(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "            dA_prev_temp = dA_prev_temp*masks['m'+str(l)]\n",
    "        else:\n",
    "            dA_prev_temp, dW_temp, db_temp = nn.linactback(grads[\"dA\" + str(l + 2)], current_cache, \"relu\")\n",
    "\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining new model with regularization parameter:........\n",
    "def llayermodel_3(X, Y, xval, yval, layers, learning_rate = 0.0007, streg = 'none', stld = 0.5, ld = 0.001, \n",
    "                reg = 'none', estop = True, max_passes = 100, num_epochs=10000, print_cost=True, printerval=1000, plotcost = True):\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "    \n",
    "    [n, m] = X.shape\n",
    "    # Parameters initialization.\n",
    "    parameters = nn.initparams(layers)\n",
    "\n",
    "    valcst = 10**5\n",
    "    passes = 0\n",
    "    oparams = parameters\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    if(streg=='dout'):\n",
    "        if (reg == 'l2reg' or reg == 'l1reg'):\n",
    "            for i in range(0, num_epochs):\n",
    "                # Forward propagation\n",
    "                a3, caches, masks = lmodelfwd_reg(X, parameters, streg, stld)\n",
    "\n",
    "                # Compute cost\n",
    "                if(plotcost):\n",
    "                    cost = compcost_reg(a3, Y, ld, parameters, reg)\n",
    "\n",
    "                # Backward propagation\n",
    "                grads = lmodelback_reg(a3, Y, caches, masks, streg)\n",
    "\n",
    "                # Update parameters\n",
    "                parameters = updateparams_reg(parameters, grads, learning_rate, ld/m, reg)\n",
    "\n",
    "                # Print the cost every 100 training example\n",
    "                if(plotcost):\n",
    "                    if print_cost and i % printerval == 0:\n",
    "                        print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                    if i % printerval == 0:\n",
    "                        costs.append(cost)\n",
    "\n",
    "                if(estop and i%10 == 0):    \n",
    "                    yvald = nn.predict(xval,parameters)\n",
    "                    vcost = compcost_reg(yvald, yval, ld, parameters, reg)\n",
    "                    if(vcost < valcst):\n",
    "                        passes = 0\n",
    "                        valcst = vcost\n",
    "                        oparams = parameters\n",
    "                    else:\n",
    "                        if(passes > max_passes):\n",
    "                            parameters = oparams\n",
    "                            print(\"breaking the loop........\")\n",
    "                            break\n",
    "                        else:\n",
    "                            passes = passes + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in range(0, num_epochs):\n",
    "                # Forward propagation\n",
    "                a3, caches, masks = lmodelfwd_reg(X, parameters, streg, stld)\n",
    "\n",
    "                # Compute cost\n",
    "                if(plotcost):\n",
    "                    cost = nn.compcost(a3, Y)\n",
    "\n",
    "                # Backward propagation\n",
    "                grads = lmodelback_reg(a3, Y, caches, masks, streg)\n",
    "\n",
    "                # Update parameters\n",
    "                parameters = nn.updateparams(parameters, grads, learning_rate)\n",
    "\n",
    "                # Print the cost every 100 training example\n",
    "                if(plotcost):\n",
    "                    if print_cost and i % printerval == 0:\n",
    "                        print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                    if i % printerval == 0:\n",
    "                        costs.append(cost)\n",
    "\n",
    "                if(estop and i%10 == 0):    \n",
    "                    yvald = nn.predict(xval,parameters)\n",
    "                    vcost = nn.compcost(yvald, yval)\n",
    "                    if(vcost < valcst):\n",
    "                        passes = 0\n",
    "                        valcst = vcost\n",
    "                        oparams = parameters\n",
    "                    else:\n",
    "                        if(passes > max_passes):\n",
    "                            parameters = oparams\n",
    "                            print(\"breaking the loop........\")\n",
    "                            break\n",
    "                        else:\n",
    "                            passes = passes + 1\n",
    "    else:\n",
    "        if (reg == 'l2reg' or reg == 'l1reg'):\n",
    "            for i in range(0, num_epochs):\n",
    "                # Forward propagation\n",
    "                a3, caches = nn.lmodelfwd(X, parameters)\n",
    "\n",
    "                # Compute cost\n",
    "                if(plotcost):\n",
    "                    cost = compcost_reg(a3, Y, ld, parameters, reg)\n",
    "\n",
    "                # Backward propagation\n",
    "                grads = nn.lmodelback(a3, Y, caches)\n",
    "\n",
    "                # Update parameters\n",
    "                parameters = updateparams_reg(parameters, grads, learning_rate, ld/m, reg)\n",
    "\n",
    "                # Print the cost every 100 training example\n",
    "                if(plotcost):\n",
    "                    if print_cost and i % printerval == 0:\n",
    "                        print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                    if i % printerval == 0:\n",
    "                        costs.append(cost)\n",
    "\n",
    "                if(estop and i%10 == 0):    \n",
    "                    yvald = nn.predict(xval,parameters)\n",
    "                    vcost = compcost_reg(yvald, yval, ld, parameters, reg)\n",
    "                    if(vcost < valcst):\n",
    "                        passes = 0\n",
    "                        valcst = vcost\n",
    "                        oparams = parameters\n",
    "                    else:\n",
    "                        if(passes > max_passes):\n",
    "                            parameters = oparams\n",
    "                            print(\"breaking the loop........\")\n",
    "                            break\n",
    "                        else:\n",
    "                            passes = passes + 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in range(0, num_epochs):\n",
    "                # Forward propagation\n",
    "                a3, caches = nn.lmodelfwd(X, parameters)\n",
    "\n",
    "                # Compute cost\n",
    "                if(plotcost):\n",
    "                    cost = nn.compcost(a3, Y)\n",
    "\n",
    "                # Backward propagation\n",
    "                grads = nn.lmodelback(a3, Y, caches)\n",
    "\n",
    "                # Update parameters\n",
    "                parameters = nn.updateparams(parameters, grads, learning_rate)\n",
    "\n",
    "                # Print the cost every 100 training example\n",
    "                if(plotcost):\n",
    "                    if print_cost and i % printerval == 0:\n",
    "                        print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                    if i % printerval == 0:\n",
    "                        costs.append(cost)\n",
    "\n",
    "                if(estop and i%10 == 0):    \n",
    "                    yvald = nn.predict(xval,parameters)\n",
    "                    vcost = nn.compcost(yvald, yval)\n",
    "                    if(vcost < valcst):\n",
    "                        passes = 0\n",
    "                        valcst = vcost\n",
    "                        oparams = parameters\n",
    "                    else:\n",
    "                        if(passes > max_passes):\n",
    "                            parameters = oparams\n",
    "                            print(\"breaking the loop........\")\n",
    "                            break\n",
    "                        else:\n",
    "                            passes = passes + 1\n",
    "\n",
    "    # plot the cost\n",
    "    if(plotcost):\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations /' + str(printerval))\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]\n",
    "params = llayermodel_3(x_str, y_str, x_val, y_val, layers, learning_rate = 0.9, streg = 'dout', stld = 0.5, ld = 0.001,\n",
    "                reg = 'none', estop = True, max_passes = 1000, num_epochs=150000, print_cost=True, printerval=10000, plotcost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = nn.predict(x_te,params)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">NOTE: why did dropout give so good result in less number of iterations because in a way drop out forces the nodes to contain independent features and not interelated data so that if a node goes off the data on other nodes not to be affected and so nodes contains may be repeated bbut independent features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Adversarial Training\n",
    "\n",
    "> Read section 7.11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Tangent Distance, Tangent Prop and ManifoldTangent Classiﬁer\n",
    "\n",
    "Say you have a image of letter A and a image where A is laterally tilted these two falls on same manifold but will out model predict them in same class(A) see a method which can help us with it.\n",
    "\n",
    "> Read section 7.13 <br> Also read this article :http://papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

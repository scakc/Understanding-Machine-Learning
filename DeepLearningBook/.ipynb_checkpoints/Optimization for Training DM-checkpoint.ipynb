{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Training Deep Models\n",
    "\n",
    "Reference book: http://www.deeplearningbook.org/contents/optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If you are familier with following concept you can skip and move forwards:\n",
    "\n",
    "    1. How Learning Diﬀers from Pure Optimization\n",
    "    2. Challenges in Neural Network Optimization\n",
    "    3. Basic Algorithms\n",
    "    4. Parameter Initialization Strategies\n",
    "    5. Algorithms with Adaptive Learning Rates\n",
    "    6. Approximate Second-Order Methods\n",
    "    7. Optimization Strategies and Meta-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0, './opt')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ann1o as ann # the nueral network model we created in previous lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How Learning Diﬀers from Pure Optimization\n",
    "Includes\n",
    "    - Emperical Risk minimization.\n",
    "    - Surrogate loss function and early stopping.\n",
    "    - Batch and minibatch Algorithms.\n",
    "    \n",
    "> Read section 8.1 here http://www.deeplearningbook.org/contents/optimization.html\n",
    "\n",
    "Before proceeding to minibatch gradient descent code let's get familier with new structure of our previous model given in file ann10.py in opt folder.... let's check if model works...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the data to fit.......\n",
    "np.random.seed(10)\n",
    "n_points = 20\n",
    "# we are limiting the output to 0 to one since the output node gives probability between 0 to 1\n",
    "X = np.linspace(0,0.8,n_points).reshape(n_points,1)\n",
    "np.random.shuffle(X)\n",
    "X = X.T\n",
    "Y = abs(X**2)+np.random.randn(1,n_points)*0.01  + 0.3\n",
    "Y = 0.8*Y/np.max(Y)\n",
    "print(\"We will try to fit a quadratic data\")\n",
    "# defining training data\n",
    "tr_size = 16\n",
    "x_tr = X[0,:tr_size].reshape(1,tr_size)\n",
    "y_tr = Y[0,:tr_size].reshape(1,tr_size)\n",
    "x_te = X[0,tr_size:].reshape(1,n_points-tr_size)\n",
    "y_te = Y[0,tr_size:].reshape(1,n_points-tr_size)\n",
    "# defining validation data\n",
    "str_size = 12\n",
    "x_str = x_tr[0,:str_size].reshape(1,str_size)\n",
    "y_str = y_tr[0,:str_size].reshape(1,str_size)\n",
    "x_val = x_tr[0,str_size:].reshape(1,tr_size-str_size)\n",
    "y_val = y_tr[0,str_size:].reshape(1,tr_size-str_size)\n",
    "plt.clf()\n",
    "plt.plot(x_str.T,y_str.T,'x',label = 'training')\n",
    "plt.plot(x_val.T,y_val.T,'x',label = 'validation')\n",
    "plt.plot(x_te.T,y_te.T,'x',label = 'testing')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 150000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 10000, \n",
    "    'plot_cost'   : True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann1 = ann.NN(layers, actis, hyperparams)\n",
    "ann1.train(x_str, y_str, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pY = ann1.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use classes helps us to extend or scale our model freely so<br>\n",
    "Now we can start minibatch gradient descent ........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minibatch gradient descent\n",
    "class mbatch_NN(ann.NN):\n",
    "    \n",
    "    def create_batches(self, x, y, k):\n",
    "        [n,m] = x.shape\n",
    "        self.batches = {}\n",
    "        siz = m//k\n",
    "        perm = np.random.permutation(m)\n",
    "        xd = x[:,perm]\n",
    "        yd = y[:,perm]\n",
    "        \n",
    "        for i in range(k):\n",
    "            if (i == k-1):\n",
    "                self.batches['bx'+ str(i)] = xd[:,i*siz:].reshape(n,-1)\n",
    "                self.batches['by'+ str(i)] = yd[:,i*siz:].reshape(n,-1)\n",
    "            else:\n",
    "                self.batches['bx'+ str(i)] = xd[:,i*siz:(i+1)*siz].reshape(n,siz)\n",
    "                self.batches['by'+ str(i)] = yd[:,i*siz:(i+1)*siz].reshape(n,siz)\n",
    "                \n",
    "    def train_batches(self, x_str,y_str,xval,yval, k):\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        \n",
    "        self.create_batches(x_str, y_str, k)\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            for j in range(k):\n",
    "                \n",
    "                X = self.batches['bx'+ str(j)]\n",
    "                Y = self.batches['by'+ str(j)]\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                if (self.plotcost):\n",
    "                    cost = self.compcost(a3, Y)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "\n",
    "                self.parameters = self.updateparams(grads, self.ld / m)\n",
    "                \n",
    "                \n",
    "            if (self.plotcost):\n",
    "                if self.print_cost and i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "            \n",
    "        self.costs.append(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 150000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 10000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann2 = mbatch_NN(layers, actis, hyperparams)\n",
    "ann2.train_batches(x_str, y_str, x_val, y_val, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pY = ann2.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Challenges in Neural Network Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would ask you to read section 8.2 from the book although it's all theoritical it includes the pitfalls of nueral network optimization which can save you in pinch so please read section 8.2 here http://www.deeplearningbook.org/contents/optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Basic Algorithms\n",
    "\n",
    "#### 3.1 Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minibatch gradient descent\n",
    "class stochastic_NN(ann.NN):\n",
    "              \n",
    "    def train_stochastic(self, x_str,y_str,xval,yval, batch_size=1, num_iter = None, init_lr = None):\n",
    "        if(num_iter == None):\n",
    "            num_iter = self.num_epochs\n",
    "        if(init_lr == None):\n",
    "            init_lr = self.learning_rate\n",
    "            \n",
    "            \n",
    "        self.learning_rate = init_lr\n",
    "        self.num_epochs = num_iter\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        olddiff = 0\n",
    "        diff = 0\n",
    "        mincost = valcst\n",
    "        \n",
    "        knb = m//batch_size\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            \n",
    "            for j in range(knb):\n",
    "                \n",
    "                lb = j*batch_size\n",
    "                ub = min(m,(j+1)*batch_size)\n",
    "                siz = ub-lb\n",
    "                X = x_str[:,lb:ub].reshape(n,siz)\n",
    "                Y = y_str[:,lb:ub].reshape(n,siz)\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "\n",
    "                self.parameters = self.updateparams(grads, self.ld / m)\n",
    "            \n",
    "            cost = self.compcost(a3, Y)\n",
    "            \n",
    "            if (mincost > cost):\n",
    "                mincost = cost\n",
    "                olddiff = diff\n",
    "                diff = cost-mincost\n",
    "                self.learning_rate = min(self.learning_rate*(np.sign(olddiff-diff)*0.01+1),0.9)\n",
    "            else:\n",
    "                olddiff = diff\n",
    "                diff = cost-mincost\n",
    "                self.learning_rate = min(self.learning_rate*(np.sign(olddiff-diff)*0.01+1),0.9)\n",
    "            \n",
    "                \n",
    "            if (self.print_cost):\n",
    "                if i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration\", i, ' is:', cost,':: l_rate',self.learning_rate)\n",
    "            if (self.plotcost):\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "            \n",
    "        self.costs.append(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 1000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann3 = stochastic_NN(layers, actis, hyperparams)\n",
    "ann3.train_stochastic(x_str, y_str, x_val, y_val,batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pY = ann3.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Momentum  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class momentum_NN(ann.NN):\n",
    "    def init_velocity(self):\n",
    "        self.velocity = {}\n",
    "        for l in range(1, self.L):\n",
    "            self.velocity['W' + str(l)] = self.parameters['W' + str(l)]*0\n",
    "            self.velocity['b' + str(l)] = self.parameters['b' + str(l)]*0\n",
    "            \n",
    "    def update_velocity(self, grads, ldbym, alpha):\n",
    "        L = len(self.parameters) // 2  # number of layers in the neural network\n",
    "        \n",
    "        # Update rule for each parameter. Use a for loop.\n",
    "        for l in range(L):\n",
    "            \n",
    "            if (self.reg == 'l2reg'):\n",
    "                self.velocity[\"W\" + str(l+1)] = alpha*self.velocity[\"W\" + str(l+1)] + (1-alpha)*(grads[\n",
    "                    \"dW\" + str(l + 1)] + ldbym*self.parameters[\"W\" + str(l + 1)])\n",
    "                \n",
    "            elif (self.reg == 'l1reg'):\n",
    "                self.velocity[\"W\" + str(l+1)] = alpha*self.velocity[\"W\" + str(l+1)]\n",
    "                - (1-alpha)*self.learning_rate*(grads[\"dW\" + str(l + 1)] + ldbym*np.sign(self.parameters[\"W\" + str(l + 1)]))\n",
    "                \n",
    "            else:\n",
    "                self.velocity[\"W\" + str(l+1)] = alpha*self.velocity[\"W\" + str(l+1)] + (1-alpha)*(grads[\n",
    "                    \"dW\" + str(l + 1)])   \n",
    "                                                                                                          \n",
    "            self.velocity[\"b\" + str(l+1)] = alpha*self.velocity[\"b\" + str(l+1)] + (1-alpha)*(grads[\n",
    "                    \"db\" + str(l + 1)]) \n",
    "            \n",
    "        l = 0\n",
    "        for l in range(L):\n",
    "            assert(self.velocity[\"W\" + str(l+1)].shape == self.parameters[\"W\" + str(l+1)].shape)\n",
    "            self.parameters[\"W\" + str(l+1)] = self.parameters[\"W\" + str(l+1)] - self.learning_rate*self.velocity[\"W\" + str(l+1)]\n",
    "            self.parameters[\"b\" + str(l+1)] = self.parameters[\"b\" + str(l+1)] - self.learning_rate*self.velocity[\"b\" + str(l+1)]\n",
    "                                                                                                          \n",
    "        return self.parameters\n",
    "    \n",
    "    def train_momentum(self, x_str,y_str,xval,yval, batch_size=1, num_iter = None, init_lr = None, alpha = 0.9):\n",
    "       \n",
    "        self.init_velocity()\n",
    "        \n",
    "        if(num_iter == None):\n",
    "            num_iter = self.num_epochs\n",
    "        if(init_lr == None):\n",
    "            init_lr = self.learning_rate\n",
    "            \n",
    "            \n",
    "        self.learning_rate = init_lr\n",
    "        self.num_epochs = num_iter\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        olddiff = 0\n",
    "        diff = 0\n",
    "        mincost = valcst\n",
    "        \n",
    "        knb = m//batch_size\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            \n",
    "            for j in range(knb):\n",
    "                \n",
    "                lb = j*batch_size\n",
    "                ub = min(m,(j+1)*batch_size)\n",
    "                siz = ub-lb\n",
    "                X = x_str[:,lb:ub].reshape(n,siz)\n",
    "                Y = y_str[:,lb:ub].reshape(n,siz)\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "                \n",
    "                self.parameters = self.update_velocity(grads, self.ld / m, alpha)\n",
    "            \n",
    "            cost = self.compcost(a3, Y)\n",
    "            \n",
    "            if (mincost > cost):\n",
    "                mincost = cost\n",
    "                olddiff = diff\n",
    "                diff = cost-mincost\n",
    "                self.learning_rate = min(self.learning_rate*(np.sign(olddiff-diff)*0.01+1),0.9)\n",
    "            else:\n",
    "                olddiff = diff\n",
    "                diff = cost-mincost\n",
    "                self.learning_rate = min(self.learning_rate*(np.sign(olddiff-diff)*0.01+1),0.9)\n",
    "            \n",
    "                \n",
    "            if (self.print_cost):\n",
    "                if i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration\", i, ' is:', cost,':: l_rate',self.learning_rate)\n",
    "            if (self.plotcost):\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "        \n",
    "        self.costs.append(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 1000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann4 = momentum_NN(layers, actis, hyperparams)\n",
    "ann4.train_momentum(x_str, y_str, x_val, y_val,batch_size = 3, alpha = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pY = ann4.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------------\n",
    "### 4. Parameter Initialization Strategies\n",
    "    \n",
    "   1. Xavier's Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class init_NN(ann.NN):\n",
    "    def xav_init_params(self):\n",
    "        np.random.seed(3)\n",
    "        for l in range(1, self.L):\n",
    "            a = np.sqrt(2/(self.layers[l] + self.layers[l - 1]))\n",
    "            self.parameters['W' + str(l)] = np.random.uniform(-a,a,size=[self.layers[l], self.layers[l - 1]])\n",
    "            self.parameters['b' + str(l)] = np.zeros((self.layers[l], 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,15,1]  # layer structure\n",
    "actis = ['none','relu', 'relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 100, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann5 = init_NN(layers, actis, hyperparams)\n",
    "ann5.train(x_str, y_str, x_val, y_val)\n",
    "cost1 = ann5.costs\n",
    "pY1 = ann5.predict(x_te)\n",
    "ann5.costs = []\n",
    "ann5.xav_init_params()\n",
    "ann5.train(x_str, y_str, x_val, y_val)\n",
    "cost2 = ann5.costs\n",
    "pY2 = ann5.predict(x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(cost1))\n",
    "plt.plot(np.squeeze(cost2))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations /' + str(ann5.printerval))\n",
    "plt.title(\"Learning rate =\" + str(ann5.learning_rate))\n",
    "plt.show()\n",
    "print(\"Predicted fit.....\")\n",
    "err1 = np.sum((pY1 - y_te)**2)/y_te.shape[1]\n",
    "err2 = np.sum((pY2 - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE1 is : \", err1)\n",
    "print(\"The MSE2 is : \", err2)\n",
    "plt.plot(x_te.T,pY1.T,'.')\n",
    "plt.plot(x_te.T,pY2.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Algorithms with Adaptive Learning Rates\n",
    "\n",
    "   1. AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class adagrad_NN(ann.NN):\n",
    "    def update_adagrad(self, grads, ldbym):\n",
    "        ep = 10**-8\n",
    "        L = len(self.parameters) // 2  # number of layers in the neural network\n",
    "        # Update rule for each parameter. Use a for loop.\n",
    "        for l in range(L):\n",
    "            self.R['W'+str(l+1)] = self.R['W'+str(l+1)] + grads['dW'+str(l+1)]**2\n",
    "            deno = np.sqrt(self.R['W' + str(l+1)]) + ep\n",
    "            if (self.reg == 'l2reg'):\n",
    "                self.parameters[\"W\" + str(l + 1)] = (1 - (self.learning_rate/deno) *ldbym) * self.parameters[\"W\" + str(l + 1)] - (self.learning_rate/deno) * grads[\n",
    "                    \"dW\" + str(l + 1)]\n",
    "            elif (self.reg == 'l1reg'):\n",
    "                self.parameters[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)] - (self.learning_rate/deno) * grads[\n",
    "                    \"dW\" + str(l + 1)] - (self.learning_rate/deno) *ldbym * np.sign(self.parameters[\"W\" + str(l + 1)])\n",
    "            else:\n",
    "                self.parameters[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)] - (self.learning_rate/deno) * grads[\"dW\" + str(l + 1)]\n",
    "\n",
    "            self.R['b'+str(l+1)] = self.R['b'+str(l+1)] + grads['db'+str(l+1)]**2\n",
    "            deno = np.sqrt(self.R['b' + str(l+1)]) + ep\n",
    "            self.parameters[\"b\" + str(l + 1)] = self.parameters[\"b\" + str(l + 1)] - (self.learning_rate/deno) * grads[\"db\" + str(l + 1)]\n",
    "            \n",
    "        return self.parameters\n",
    "    \n",
    "    def train_adagrad(self, x_str,y_str,xval,yval, batch_size=None, num_iter = None, init_lr = None):\n",
    "        self.R = {}\n",
    "        for l in range(1, self.L):\n",
    "            self.R['W' + str(l)] = np.zeros((self.layers[l], self.layers[l - 1]))\n",
    "            self.R['b' + str(l)] = np.zeros((self.layers[l], 1))\n",
    "            \n",
    "        \n",
    "        if(num_iter == None):\n",
    "            num_iter = self.num_epochs\n",
    "        if(init_lr == None):\n",
    "            init_lr = self.learning_rate\n",
    "        if(batch_size == None):\n",
    "            batch_size = x_str.shape[1]\n",
    "            \n",
    "            \n",
    "        self.learning_rate = init_lr\n",
    "        self.num_epochs = num_iter\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        olddiff = 0\n",
    "        diff = 0\n",
    "        mincost = valcst\n",
    "        \n",
    "        knb = m//batch_size\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            \n",
    "            for j in range(knb):\n",
    "                \n",
    "                lb = j*batch_size\n",
    "                ub = min(m,(j+1)*batch_size)\n",
    "                siz = ub-lb\n",
    "                X = x_str[:,lb:ub].reshape(n,siz)\n",
    "                Y = y_str[:,lb:ub].reshape(n,siz)\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "\n",
    "                self.parameters = self.update_adagrad(grads, self.ld / m)\n",
    "            \n",
    "            cost = self.compcost(a3, Y)\n",
    "        \n",
    "            \n",
    "                \n",
    "            if (self.print_cost):\n",
    "                if i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration\", i, ' is:', cost)\n",
    "            if (self.plotcost):\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "            \n",
    "        self.costs.append(costs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 500, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 1000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann6 = adagrad_NN(layers, actis, hyperparams)\n",
    "ann6.train_adagrad(x_str, y_str, x_val, y_val, batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = ann6.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rmsprop_NN(ann.NN):\n",
    "    def update_rmsprop(self, grads, ldbym):\n",
    "        ep = 10**-6\n",
    "        L = len(self.parameters) // 2  # number of layers in the neural network\n",
    "        # Update rule for each parameter. Use a for loop.\n",
    "        for l in range(L):\n",
    "            self.R['W'+str(l+1)] = self.rho*self.R['W'+str(l+1)] + (1-self.rho)*grads['dW'+str(l+1)]**2\n",
    "            deno = np.sqrt(self.R['W' + str(l+1)] + ep)\n",
    "            if (self.reg == 'l2reg'):\n",
    "                self.parameters[\"W\" + str(l + 1)] = (1 - (self.learning_rate/deno) *ldbym) * self.parameters[\"W\" + str(l + 1)] - (self.learning_rate/deno) * grads[\n",
    "                    \"dW\" + str(l + 1)]\n",
    "            elif (self.reg == 'l1reg'):\n",
    "                self.parameters[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)] - (self.learning_rate/deno) * grads[\n",
    "                    \"dW\" + str(l + 1)] - (self.learning_rate/deno) *ldbym * np.sign(self.parameters[\"W\" + str(l + 1)])\n",
    "            else:\n",
    "                self.parameters[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)] - (self.learning_rate/deno) * grads[\"dW\" + str(l + 1)]\n",
    "\n",
    "            self.R['b'+str(l+1)] = self.R['b'+str(l+1)] + grads['db'+str(l+1)]**2\n",
    "            deno = np.sqrt(self.R['b' + str(l+1)] + ep)\n",
    "            self.parameters[\"b\" + str(l + 1)] = self.parameters[\"b\" + str(l + 1)] - (self.learning_rate/deno) * grads[\"db\" + str(l + 1)]\n",
    "            \n",
    "        return self.parameters\n",
    "    \n",
    "    def train_rmsprop(self, x_str,y_str,xval,yval,rho = 0.5, batch_size=None, num_iter = None, init_lr = None):\n",
    "        self.R = {}\n",
    "        self.rho = rho\n",
    "        for l in range(1, self.L):\n",
    "            self.R['W' + str(l)] = np.zeros((self.layers[l], self.layers[l - 1]))\n",
    "            self.R['b' + str(l)] = np.zeros((self.layers[l], 1))\n",
    "            \n",
    "        \n",
    "        if(num_iter == None):\n",
    "            num_iter = self.num_epochs\n",
    "        if(init_lr == None):\n",
    "            init_lr = self.learning_rate\n",
    "        if(batch_size == None):\n",
    "            batch_size = x_str.shape[1]\n",
    "            \n",
    "            \n",
    "        self.learning_rate = init_lr\n",
    "        self.num_epochs = num_iter\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        olddiff = 0\n",
    "        diff = 0\n",
    "        mincost = valcst\n",
    "        \n",
    "        knb = m//batch_size\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            \n",
    "            for j in range(knb):\n",
    "                \n",
    "                lb = j*batch_size\n",
    "                ub = min(m,(j+1)*batch_size)\n",
    "                siz = ub-lb\n",
    "                X = x_str[:,lb:ub].reshape(n,siz)\n",
    "                Y = y_str[:,lb:ub].reshape(n,siz)\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "\n",
    "                self.parameters = self.update_rmsprop(grads, self.ld / m)\n",
    "            \n",
    "            cost = self.compcost(a3, Y)\n",
    "        \n",
    "            \n",
    "                \n",
    "            if (self.print_cost):\n",
    "                if i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration\", i, ' is:', cost)\n",
    "            if (self.plotcost):\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "            \n",
    "        self.costs.append(costs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 500, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 1000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann7 = rmsprop_NN(layers, actis, hyperparams)\n",
    "ann7.train_rmsprop(x_str, y_str, x_val, y_val, rho = 0.5, batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = ann7.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

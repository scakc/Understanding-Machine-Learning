{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Probability and Information Theory\n",
    "\n",
    "<br>\n",
    "\n",
    "Probability theory is a mathematical framework for representing uncertainstatements. Probaility in simple terms certainity or uncertainity of anything.\n",
    "\n",
    "If you are familier with following concepts you can move to next notebook:\n",
    ">Reference http://www.deeplearningbook.org/contents/prob.html\n",
    "    \n",
    "    1. Why Probability\n",
    "    2. Random Variables\n",
    "    3. Probability Distributions\n",
    "    4. Marginal Probabilty\n",
    "    5. Conditional Probabilty\n",
    "    6. The Chain Rule of Conditional Probabilities\n",
    "    7. Independence and Conditional Independence\n",
    "    8. Expectation, Variance and Covariance\n",
    "    9. Common Probability Distributions\n",
    "    10. Useful Properties of Common Functions\n",
    "    11. Bayes’ Rule\n",
    "    12. Technical Details of Continuous Variables\n",
    "    13. Information Theory\n",
    "    14. Structured Probabilistic Models\n",
    "    \n",
    "----------------------\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#library imports\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Why Probability\n",
    "\n",
    "When we want to make a program to do a certain thing we usually know what kinds of input will come and what output will be but in machine learning and also in our brains we compare things and decide the certainity and uncertaininty of things differently.\n",
    "For example i show a animal species you haven't seen before you will try to relate it to a creature that you have seen. Like if you see a Liger(Hybrid of Lion and Tiger). You will think how much it acts/looks like a lion and how much like a tiger so this how much is what we call probility of that feature or event etc. Machine Learning uses alot of probability theory to decide things like that and this increases the power of machine mind from logic to extended and uncertain logic.\n",
    "\n",
    "There are 3 possible sources of uncertaininty:\n",
    "    \n",
    "    - Undecidable(Stochastic) system modeling\n",
    "    - Incomplete Observation\n",
    "    - Incomplete model\n",
    "    \n",
    "In many cases it's easy to use a small uncertainity rather than a large complex certain one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some terms:\n",
    "    \n",
    "***Degree of Belief:*** Varies from 0(absolute certainity being false) to 1(absolute certainity being true).\n",
    "\n",
    "***Frequentist Probability:*** It is an interpretation of probability, it defines an event's probability as the limit of its relative frequency in a large number of trials\n",
    "\n",
    "***Bayesian Probability:*** Probability is interpreted as reasonable expectation representing a state of knowledge or as\n",
    "quantification of a personal belief.\n",
    "\n",
    ">Probability can be seen as the extension of logic to deal with uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Example\n",
    "## To find what is certainity of you being concentrated on this chapter?\n",
    "\n",
    "## YOUR CODE HERE:\n",
    "max_dof = None  # write maximum limit of Degree of bilief\n",
    "#END\n",
    "\n",
    "if(math.pow(max_dof,max_dof) == max_dof and max_dof > 0):\n",
    "    print(\"Good! you have good concentration.\")\n",
    "else:\n",
    "    print(\"You are not concentrating even a bit!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Expected output : Good! you have good concentration.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Variables\n",
    "\n",
    "A random variable is a variable that can take on diﬀerent values randomly. Random variables may be discrete or continuous. A discrete random variable is one that has a ﬁnite or countably inﬁnite number of states. Note that these states are not necessarily the integers, they can also just be named states that are not considered to have any numerical value. A continuous random variable is associated with a real value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Probability Distributions\n",
    "\n",
    "A probability distribution is a description of how likely a random variable or set of random variables is to take on each of its possible states. The way we describe probability distributions depends on whether the variables are discrete or continuous.\n",
    "\n",
    "----------------------\n",
    "\n",
    "#### 3.1 Discrete Variables and Probability Mass Functions\n",
    "Probabilties for a set of discrete random variables in being in a set of particular states respectively can be measured using a probabilty mass function (PMF).\n",
    "\n",
    "The probability mass function maps from a state of a random variable tothe probability of that random variable taking on that state.\n",
    "\n",
    "A PMF must satisfy following criteria:\n",
    "\n",
    "   - The domain of P must be the set of all possible states of x.\n",
    "   - Probabilty of each state for x must be between 0 and 1\n",
    "   - For all possible states of a random variable the sum of probabilities must go to 1.0\n",
    "\n",
    "> Example:\n",
    "```\n",
    "A = {1,9,4,7,4,3,1,4,0,8}\n",
    "P(4) = (frequency of 4 in A)/(length of A)  --> PMF  = frequency/no_samples\n",
    "     = 3/10\n",
    "     = 0.3 or 30%\n",
    "```\n",
    "\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Continuous Variables and Probability Density Functions\n",
    "\n",
    "When working with continuous random variables, we describe probability distributions using a probability density function (PDF) rather than a probability mass function.\n",
    "\n",
    "A PDF must satisfy following criteria:\n",
    "    \n",
    "   - The domain of P must be the set of all possible states of x.\n",
    "   - Probabilty of each state for x must be greater than 0.\n",
    "   - ∫(p(x)dx) = 1. Integration must be equal to 1\n",
    "    \n",
    "    \n",
    "A probability density functionp(x) does not give the probability of a speciﬁc state directly, instead the probability of landing inside an inﬁnitesimal region with volume δx is given by p(x)δx.\n",
    "\n",
    "> Example: <img src='prob/pdf.gif'><center>A Gaussian PDF of random variable Length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example Probability distribution:\n",
    "# PMD:\n",
    "def pmd(x,A):\n",
    "    \n",
    "    ## YOUR CODE HERE:\n",
    "    freq = None # use np.sum(A==x),  A == x gives a array of ones and zeros where if A[i] == x --> 1 \n",
    "    l1   = None # use A.shape[0] to calculate length of A\n",
    "    pxA  = None # freq/l1\n",
    "    #END\n",
    "    \n",
    "    return pxA\n",
    "\n",
    "# A Gaussian PDF , Gauss was a german mathematician\n",
    "def gaussian_pdf(x):\n",
    "    mu = np.mean(x)\n",
    "    sig = np.std(x)\n",
    "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "\n",
    "\n",
    "A = np.array([1,0,2,30,2,1,22,1,5,6,3,8,6,1,2,6,1,34,1])\n",
    "pmd_1 = pmd(1,A)\n",
    "print(pmd_1)\n",
    "\n",
    "A = np.arange(-10,10,0.001)\n",
    "pdf_A = gaussian_pdf(A)\n",
    "plt.plot(A,pdf_A)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Expected Output \n",
    "```\n",
    "   0.315789473684\n",
    "```\n",
    "   <img src=\"prob/pdf_A.png\">\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Marginal Probability\n",
    "\n",
    "Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as the marginal probability distribution.\n",
    "\n",
    "For example, suppose we have discrete random variables x and y, and we know Pr(x, y). We can ﬁnd Pr(x) with the sum rule:\n",
    "\n",
    "$$ {\\Pr(X=x)=\\sum _{y}\\Pr(X=x,Y=y)} $$\n",
    "\n",
    "Similarly for continuous random variables, the marginal probability density function can be written as p<sub>X</sub>(x). This is\n",
    "\n",
    "$${p_{X}(x)=\\int _{y}p_{X,Y}(x,y)\\,\\mathrm {dy}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Example: <img src=\"prob/mdist.jpg\">\n",
    "Each Cell in inner light skyblue colored table represents a Pr(X=x,Y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### 5. Conditional Probabilty\n",
    "\n",
    "In many cases, we are interested in the probability of some event, given that some other event has happened. This is called a conditional probability\n",
    "\n",
    "> **P(X=x given Y=y) = P(X=x, Y=y) / P(Y=y)**, in mathematical symbols$$P(x|y)={\\frac {P(x\\cap y)}{P(y)}}$$\n",
    "\n",
    "It is only defined when P(Y=y) > 0\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. The Chain Rule of Conditional Probabilities\n",
    "\n",
    "You would have observed that if we apply condition probability rule to a n-variabled problem we can extend the probability function something like this:\n",
    "\n",
    "$$\\mathrm {P} (A_{4},A_{3},A_{2},A_{1})=\\mathrm {P} (A_{4}\\mid A_{3},A_{2},A_{1})\\cdot \\mathrm {P} (A_{3}\\mid A_{2},A_{1})\\cdot \\mathrm {P} (A_{2}\\mid A_{1})\\cdot \\mathrm {P} (A_{1})$$\n",
    "<center>or u can write like this</center>\n",
    "$$\\mathrm {P} (A_{4} \\cap A_{3} \\cap A_{2} \\cap A_{1})=\\mathrm {P} (A_{4}\\mid A_{3} \\cap A_{2} \\cap A_{1})\\cdot \\mathrm {P} (A_{3}\\mid A_{2} \\cap A_{1})\\cdot \\mathrm {P} (A_{2}\\mid A_{1})\\cdot \\mathrm {P} (A_{1})$$\n",
    "\n",
    "> Think a bit you will get it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Probabilty read and run this cell\n",
    "#definition--------------------------------------------------------------------\n",
    "A1 = np.array([1,2,5,1,1,6,1,8,1,0,1,1])\n",
    "A2 = np.array([4,2,3,8,4,6,4,8,4,0,4,4])\n",
    "A3 = np.array([2,5,8,4,2,6,2,8,2,0,2,4])\n",
    "A4 = np.array([1,5,3,4,5,6,1,8,1,0,1,4])\n",
    "\n",
    "m = A1.shape[0] # length of matrices\n",
    "\n",
    "#-------------------------------------------------------------------------------\n",
    "# function to operate on two array sets and return a array if element is present put 1 else 0 and a probabilty\n",
    "def pr_xy(x,y,A,B):\n",
    "    Ax = (A==x)\n",
    "    By = (B==y)\n",
    "    Cxy = np.logical_and(Ax,By)\n",
    "    return Cxy\n",
    "\n",
    "\n",
    "#In this section-----------------------------------------------------------------\n",
    "# we will find probability p(A4 = 1, A3 = 2, A2 = 4, A1 = 1):\n",
    "pA1 = np.sum(A1 == 1)/m                     # p(A1 = 1)\n",
    "\n",
    "A2nA1 = pr_xy(1,4,A1,A2)\n",
    "pA2nA1 = np.sum(A2nA1)/m                    # p(A1 = 1 , A2 = 4)\n",
    "pA2_A1 = (pA2nA1/pA1)                       # p(A2 | A1)\n",
    "\n",
    "A3nA2nA1 = pr_xy(1,2,A2nA1,A3)\n",
    "pA3nA2nA1 = np.sum(A3nA2nA1)/m              # p(A1 = 1, A2 = 4, A3 = 2)\n",
    "pA3_A2nA1 = (pA3nA2nA1/pA2nA1)              # p(A3 | A2,A1)\n",
    "\n",
    "A4nA3nA2nA1 = pr_xy(1,1,A3nA2nA1,A4)\n",
    "pA4nA3nA2nA1 = np.sum(A4nA3nA2nA1)/m        # p(A4 = 1, A3 = 2, A2 = 4, A1 = 1)\n",
    "pA4_A3nA2nA1 = (pA4nA3nA2nA1/pA3nA2nA1)     # p(A4 | A3,A2,A1)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "# Verification of conditional prob:\n",
    "chain = (pA4_A3nA2nA1*pA3_A2nA1*pA2_A1*pA1)\n",
    "assert(chain == pA4nA3nA2nA1)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "print(\"P(A4,A3,A2,A1)=\" + str(pA4nA3nA2nA1)+\"\")\n",
    "print(\"P(A4,A3,A2,A1)=P(A4∣A3,A2,A1)⋅P(A3∣A2,A1)⋅P(A2∣A1)⋅P(A1)\")\n",
    "print(\"              =\"+str(pA4_A3nA2nA1)+\"*\"+str(pA3_A2nA1)+\"*\"+str(pA2_A1)+\"*\"+str(pA1))\n",
    "print(\"              =\"+str(chain)+\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### 7. Independence and Conditional Independence\n",
    "\n",
    "Two random variables x and y are independent if their probability distribution can be expressed as a product of two factors, one involving only x and one involving only y:\n",
    "\n",
    "$$\\mathrm{P}(x \\cap y) = \\mathrm{P}(x)\\mathrm{P}(y).$$\n",
    "\n",
    "Two random variables x and y are conditionally independent given a random variable z if the conditional probability distribution over x and y factorizes in this way for every value of z:\n",
    "\n",
    "$$\\mathrm{P}(x \\cap y | z) = \\mathrm{P}(x | z)\\mathrm{P}(y | z).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 8. Expectation, Variance and Covariance\n",
    "#### Expectation\n",
    "The expectation or expected value of some function f(x) with respect to a probability distribution P(x) is the average, or mean value that f takes on when x is drawn from P.\n",
    "\n",
    "For discrete random variable\n",
    "\n",
    "$${\\displaystyle \\operatorname {E} [fX]=fx_{1}p_{1}+fx_{2}p_{2}+\\cdots +fx_{k}p_{k}}$$\n",
    "\n",
    "For continous random variable\n",
    "\n",
    "$${\\displaystyle \\operatorname {E} [fX]=\\int f{_x}p(x)\\,dx.}$$\n",
    "\n",
    "#### Variance\n",
    "The variance gives a measure of how much the values of a function of a random variable x vary as we sample diﬀerent values of x from its probability distribution:\n",
    "\n",
    "$$\\operatorname {Var} (fX)=\\operatorname {E} \\left[(fX-\\mu )^{2}\\right]$$\n",
    "$$\\mu = {E} [fX]$$\n",
    "\n",
    "When the variance is low, the values of f(x) cluster near their expected value. The square root of the variance is known as the **standard deviation**.\n",
    "\n",
    "#### Covariance\n",
    "\n",
    "The covariance gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:\n",
    "\n",
    "$$\\operatorname {cov} (X,Y)=\\operatorname {E} {{\\big [}(X-\\operatorname {E} [X])(Y-\\operatorname {E} [Y]){\\big ]}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Expectation, Variance and Covariance\n",
    "x = np.array([1,8,9,2,1,0,1,2,-3,-2,-9,-11,2,1,-1])\n",
    "A = x**2 + 3\n",
    "B = (x) - 10\n",
    "C = x**2 - 5\n",
    "\n",
    "def mean(X):\n",
    "    ## YOUR CODE HERE\n",
    "    mu = None # considering probabilty of x is 1/N so you can use --> np.sum(X,axis=0)/X.shape[0]\n",
    "    #END\n",
    "    return mu\n",
    "\n",
    "def vrns(X):\n",
    "    mu = mean(X)\n",
    "    ## YOUR CODE HERE\n",
    "    var = None # read and apply the equation shown above as follows --> mean(np.square(X - mu))\n",
    "    #END\n",
    "    return var\n",
    "    \n",
    "def covr(X, Y):\n",
    "    mux = mean(X)\n",
    "    muy = mean(Y)\n",
    "    ## YOUR CODE HERE\n",
    "    cov = None # remember variance is a spl. case of covariance with itself so use --> mean((X-mux)*(Y-muy))\n",
    "    #END\n",
    "    return cov\n",
    "\n",
    "mx = mean(x)\n",
    "va = vrns(A)\n",
    "vb = vrns(B)\n",
    "cac = covr(A, C)\n",
    "cab = covr(A, B)\n",
    "\n",
    "\n",
    "print(\"The mean of x :\",mx)\n",
    "print(\"The variance of A :\",va)\n",
    "print(\"The variance of B :\",vb)\n",
    "print(\"The Covariance of A,C :\",cac)\n",
    "print(\"The Covariance of A,B :\",cab)\n",
    "print(\"Note if Covariance is high then the two sets are highly correalted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Expected Output:\n",
    "```\n",
    "    The mean of x : 0.0666666666667\n",
    "    The variance of A : 1502.24888889\n",
    "    The variance of B : 25.1288888889\n",
    "    The Covariance of A,C : 1502.24888889\n",
    "    The Covariance of A,B : -56.8088888889\n",
    "    Note if Covariance is high then the two sets are highly correalted\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Common Probability Distributions\n",
    "\n",
    "#### 9.1 Bernoulli Distribution\n",
    "\n",
    "If X is a random variable with this distribution, we have:\n",
    "\n",
    "$${\\displaystyle \\Pr(X=1)=p =1-\\Pr(X=0)=1-q.}$$\n",
    "\n",
    "The PMF is defined as $${\\displaystyle f(k;p)=p^{k}(1-p)^{1-k}\\!\\quad {\\text{for }}k\\in \\{0,1\\}}$$\n",
    "\n",
    "--------------------------\n",
    "\n",
    "#### 9.2 Multinoulli Distribution\n",
    "\n",
    "The multinoulli or categorical distribution is a distribution over a single discrete variable with k diﬀerent states, where k is ﬁnite.\n",
    "\n",
    "----------------\n",
    "\n",
    "#### 9.3 Gaussian Distribution\n",
    "\n",
    "As we have discussed earlier the most commonly used distribution over real numbers is the normal distribution, also known as the Gaussian distribution:\n",
    "\n",
    "The PDF is given by: $${\\displaystyle f(x\\;|\\;\\mu ,\\sigma ^{2})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\;e^{-{\\frac {(x-\\mu )^{2}}{2\\sigma ^{2}}}}}$$ where , $$\\mu = mean,\\sigma = standard-deviation$$\n",
    "\n",
    "> Read datailed explanation for n-dimensional generalization of this distribution here: http://www.deeplearningbook.org/contents/prob.html section 3.9.3\n",
    "\n",
    "----------------------\n",
    "\n",
    "#### 9.4 Exponential and Laplace Distributions\n",
    "\n",
    "In the context of deep learning, we often want to have a probability distribution with a sharp point at x= 0. To accomplish this, we can use the **exponential distribution**:\n",
    "\n",
    "$$p(x; λ) = λ.exp (−λx);x >= 0\\\\ = 0 ; x < 0$$\n",
    "\n",
    "A closely related probability distribution that allows us to place a sharp peak of probability mass at an arbitrary point µ is the **Laplace distribution**:\n",
    "\n",
    "$$Laplace(x; µ, γ) = {1 \\over 2γ} .exp \\left(−{|x − µ|\\overγ}\\right)$$\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "#### 9.5 The Dirac Distribution and Empirical Distribution\n",
    "\n",
    "In some cases, we wish to specify that all the mass in a probability distribution clusters around a single point. This can be accomplished by deﬁning a PDF using the Dirac delta function, δ(x):\n",
    "\n",
    "$$p(x) = δ(x −µ)$$\n",
    "\n",
    "> Dirac delta function is a special function which is very high at 0 and rapidly decreases to zero in its neighbour hood. In other word its like a peak at it's 0.<br>If you wanna read in more detail check wikipedia https://en.wikipedia.org/wiki/Dirac_delta_function\n",
    "\n",
    "\n",
    "A common use of the Dirac delta distribution is as a component of an **empirical distribution**,\n",
    "\n",
    "$$p(x) ={1\\over m}\\sum^m_{ i=1}δ(x −x(i))$$\n",
    "\n",
    "which puts probability mass 1/m on each of the m points x(1), . . . , x(m), forming a given data set or collection of samples. The Dirac delta distribution is only necessary to deﬁne the empirical distribution over continuous variables. For discrete variables, the situation is simpler: an empirical distribution can be conceptualized as a multinoulli distribution, with a probability associated with each possible input value that is simply equal to the empirical frequency of that value in the training set.\n",
    "\n",
    "-----------------------------\n",
    "#### 9.6 Mixtures of Distributions\n",
    "\n",
    "It is also common to deﬁne probability distributions by combining other simpler probability distributions. One common way of combining distributions is to construct a mixture distribution.\n",
    "\n",
    "For example look as this a countable finite mixture distribution:\n",
    "$$f(x)=\\sum _{i=1}^{n}\\,w_{i}\\,p_{i}(x).$$\n",
    "\n",
    "such that wi ≥ 0 and ∑wi = 1\n",
    "\n",
    "An uncountable mixture distribution:\n",
    "\n",
    "$$f(x)=\\int _{A}\\,w(a)\\,p(x;a)\\,da$$\n",
    "\n",
    "> For more detailed study of mixture distribution you can look here: https://en.wikipedia.org/wiki/Mixture_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability Distribution\n",
    "# plot arrangements\n",
    "f, mpl = plt.subplots(2, 2, figsize=[14,7])\n",
    "# Different distribution are as follows: -\n",
    "\n",
    "# Bernoulli Distribution\n",
    "Ab = np.array([0,1,1,0,1,1,1,0,0,0,0,1,1,1,0,1,1]) # data\n",
    "\n",
    "## YOUR CODE HERE\n",
    "pb1 = None # probability of one = (number of ones / total length ) use --> np.sum(Ab,axis=0)/Ab.shape[0] \n",
    "#END\n",
    "\n",
    "pb0 = 1 - pb1 #probability o zeros is 1 - (probabilitiy of 1)\n",
    "print(\"The Bernoulli Probability of 1 : \",pb1,\"\\nThe Bernoulli Probability of 0 : \",pb0)\n",
    "#---------------------------\n",
    "# Exponential Distribution\n",
    "Ae = np.arange(-10,100,0.01) # random variable data \n",
    "lmda = 0.2 # lambda for distribution\n",
    "\n",
    "## YOUR CODE HERE\n",
    "pe = None # use --> lmda*np.exp(-lmda*Ae)\n",
    "#END\n",
    "\n",
    "pe = pe*(Ae>=0) # masking negative values of Ae to zero\n",
    "mpl[0,0].plot(Ae,pe)\n",
    "mpl[0,0].set_title(\"The exponential distribution\")\n",
    "#---------------------------\n",
    "# Laplace distribution\n",
    "me = np.mean(Ae)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "pl = None # use --> (1/2/lmda)*lmda*np.exp(-np.abs(Ae-me))\n",
    "#END\n",
    "\n",
    "mpl[0,1].plot(Ae,pl)\n",
    "mpl[0,1].set_title(\"The Laplace distribution\")\n",
    "#----------------------------\n",
    "# Emperical distribution\n",
    "#dirac delta fucntion\n",
    "def diracdel(x,a):\n",
    "    dxa = (1/np.abs(a)/np.sqrt(np.pi))*np.exp(-np.square(x/a))\n",
    "    return dxa\n",
    "\n",
    "a = 0.01 # make it near to zero to get exact curves \n",
    "# according to the definition of emperical distribution\n",
    "pemp = (diracdel(Ae,a) + diracdel(Ae-20,a) + diracdel(Ae-30,a) + diracdel(Ae-90,a))/4\n",
    "mpl[1,0].plot(Ae,pemp)\n",
    "mpl[1,0].set_title(\"The Emperical distribution\")\n",
    "#-----------------------------\n",
    "# Mixture Distribution\n",
    "def gauss(x,mu,sig):\n",
    "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "\n",
    "# parameters for mixture distribution\n",
    "W1 = 0.3\n",
    "G1 = gauss(Ae,20,6) # distribution 1\n",
    "W2 = 0.4\n",
    "G2 = gauss(Ae,40,6) # distribution 2\n",
    "W3 = 0.3\n",
    "G3 = gauss(Ae,60,6) # distribution 3\n",
    "\n",
    "pmix = W1*G1 + W2*G2 + W3*G3 # sum of weighted distributions\n",
    "mpl[1,1].plot(Ae,pmix)\n",
    "mpl[1,1].set_title(\"The Mixture distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Expected output:\n",
    "```\n",
    "The Bernoulli Probability of 1 :  0.588235294118 \n",
    "The Bernoulli Probability of 0 :  0.411764705882\n",
    "```\n",
    "<img src=\"prob/distributions.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Useful Properties of Common Functions\n",
    "\n",
    "Certain functions arise often while working with probability distributions, especially the probability distributions used in deep learning models.\n",
    "\n",
    "Common Examples:\n",
    "\n",
    "**Logistic Sigmoid:**$${\\displaystyle S(x)={\\frac {1}{1+e^{-x}}}={\\frac {e^{x}}{e^{x}+1}}}$$\n",
    "\n",
    "The logistic sigmoid is has range between 0 and one.  It saturates when its argument is very positive or very negative, meaning that the function becomes very ﬂat and insensitive to small changes in its input.\n",
    "\n",
    "**Softplus function:**$${\\displaystyle f(x)=\\ln[1+\\exp(x)]}$$\n",
    "\n",
    "The name of the softplus function comes from the fact that it is a smoothed, or “softened,” version of Rectifier linear unit **ReLU**\n",
    "\n",
    "**ReLU function:**$${\\displaystyle f(x)=x^{+}=\\max(0,x)}$$\n",
    "\n",
    "This function acts as a rectifier for negative values.\n",
    "\n",
    ">NOTE: If you are familiar with calculation of Integrals and Dervatives of a given function you should find the derivatives of above function it will be useful later on. If you aree not familier then it would be good if you learn it. It will affect a great deal of understanding in many fields if you know those concepts.\n",
    "Look for small lectures on **Google**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common Fucntions\n",
    "# Run this cell to see plots of those functions\n",
    "def sigmoid(X):\n",
    "    return(1/(1+np.exp(-x)))\n",
    "def softplus(X):\n",
    "    return(np.log(1+np.exp(x)))\n",
    "def relu(X):\n",
    "    msk = X>=0\n",
    "    return X*msk\n",
    "\n",
    "x = np.arange(-10,10,0.1)\n",
    "sig_x = sigmoid(x)\n",
    "sof_x = softplus(x)\n",
    "rel_x = relu(x)\n",
    "\n",
    "f, ax = plt.subplots(1,3, figsize=[20,4])\n",
    "ax[0].plot(x,sig_x)\n",
    "ax[0].set_title(\"Sigmoid\")\n",
    "ax[1].plot(x,sof_x)\n",
    "ax[1].set_title(\"Softplus\")\n",
    "ax[2].plot(x,rel_x)\n",
    "ax[2].set_title(\"ReLU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Bayes' Rule\n",
    "\n",
    "We often ﬁnd ourselves in a situation where we know P(B | A) and need to know P(A | B). Fortunately, if we also know P(A), we can compute the desired quantity using **Bayes’ rule:**\n",
    "\n",
    "$${\\displaystyle P(A\\mid B)={\\frac {P(B\\mid A)\\,P(A)}{P(B)}}}$$\n",
    "\n",
    "Note that while P(B) appears in the formula, it is usually feasible to compute \n",
    "\n",
    "$$P (B) =\\sum_AP (B | A)P (A)$$\n",
    "\n",
    "so we do not need to begin with knowledge of P (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### 12. Technical Details of Continuous Variables\n",
    "\n",
    "A detailed understanding of some properties of random continous varible in probability theory.\n",
    "\n",
    "> Read Section 3.12 here http://www.deeplearningbook.org/contents/prob.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Information Theory\n",
    "\n",
    "Information theory is a branch of applied mathematics that revolves around quantifying how much information is present in a signal.\n",
    "\n",
    "The basic intuition behind information theory is that learning an unlikely event has occurred is more informative than learning that a likely event has occurred. A message saying “the sun rose this morning” is so uninformative as to be unnecessary to send, but a message saying “there was a solar eclipse this morning” is very informative.\n",
    "\n",
    "In other words the requirement is:\n",
    "    - likely event should have low information content.\n",
    "    - unlikely event should have high information content.\n",
    "    - independent events should have additive information.\n",
    "    \n",
    "So, we define a term ** Self Information :**\n",
    "$$I(x) = −log_eP (x)$$\n",
    "\n",
    "Self-information deals only with a single outcome.\n",
    "\n",
    "To know expected amount of infromation in a distribution we can use:\n",
    "\n",
    "** Shannon Distribution:**\n",
    "$$H(x) = E_{x∼P}[I(x)] = −E_{x∼P}[log P (x)]$$\n",
    "<center>or</center>$${\\displaystyle \\mathrm {H} (X)=\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\,\\mathrm {I} (x_{i})}=-\\sum _{i=1}^{n}{\\mathrm {P} (x_{i})\\log _{e}\\mathrm {P} (x_{i})},}$$\n",
    "When x is continuous, the Shannon entropy is known as the diﬀerential entropy.\n",
    "\n",
    "If we have two separate probability distributions P(x) and Q(x) over the same random variable x, we can measure how diﬀerent these two distributions are using the **Kullback-Leibler (KL) divergence:**\n",
    "\n",
    "$$D_{KL}(P||Q) = E_{x∼P}[log{P (x)\\over Q(x)}]$$\n",
    "\n",
    "A quantity that is closely related to the KL divergence is the cross-entropy:\n",
    "\n",
    "$$H(P, Q) = −E_{x∼P}[log Q(x)]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot shannon run this cell\n",
    "def shannon(P):\n",
    "    info1 = -np.log(P   + 0.000000000000000001)\n",
    "    info2 = -np.log(1-P + 0.000000000000000001)\n",
    "    shan1 = P*info1\n",
    "    shan2 = (1-P)*info2\n",
    "    shan = shan1 + shan2\n",
    "    return shan\n",
    "print(\"\\nShannon Entropy plot of a binary random variable being 0 or 1\")\n",
    "p = np.arange(0,1,0.001) # probability outcomes\n",
    "s = shannon(p)\n",
    "plt.plot(p,s)\n",
    "plt.xlabel(\"P\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">See Figure : When p is near 0, the distribution is nearly deterministic, because the random variable is nearly always 0. When p is near 1, the distribution is nearly deterministic, because the random variable is nearly always 1. When p= 0.5, the entropy is maximal, because the distribution is uniform over the two outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Structured Probabilistic Models\n",
    "\n",
    "> This part is very intuitive so i advise you to read section 3.14 from http://www.deeplearningbook.org/contents/prob.html \n",
    "\n",
    "\n",
    "---------------------\n",
    "> ** Congrats on completing second tutorial in this series! Keep Moving **\n",
    "\n",
    "In next part we will discuss the Numerical Computation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familier with following concepts you can move to next notebook:\n",
    ">Reference http://www.deeplearningbook.org/contents/linear_algebra.html\n",
    "\n",
    "    1. Scalar,Vector,Matrix and Tensors\n",
    "    2. Multiplication of matrices and vectors\n",
    "    3. Identity and Inverse Matrices\n",
    "    4. Linear Dependence and span\n",
    "    5. Nomrs\n",
    "    6. Special Matrices and Vectors\n",
    "    7. Eigen Decomposition\n",
    "    8. Singular Value Decompositon\n",
    "    9. The Moore-Penrose Pseudoinverse\n",
    "    10. Trace Operator\n",
    "    11. Example: Principal Component Analysis\n",
    "    \n",
    "   -------------------\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#library imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalars are single numbers:\n",
    ">Examples\n",
    "```\n",
    "    A = 10\n",
    "    B = -10\n",
    "    C = 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scalar example in python\n",
    "## Your Code here\n",
    "scalar_a  = None # change it from None to scalar value 10.5\n",
    "## End\n",
    "print(\"Printing a Scalar Value:\", scalar_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Expected Output: Printing a Scalar Value: 10.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are represented as array of numbers/Scalars.\n",
    "\n",
    "We will use **numpy**(a powerful parallel computation python library for many application in numerical computations.) to operate on n dimensional arrays\n",
    "\n",
    ">Examples\n",
    "```\n",
    "A = [0,10,-2.5,...,21]   A ROW VECTOR\n",
    "B = [[0]               \n",
    "     [10]\n",
    "     [-2.5]              A COLUMN VECTOR\n",
    "     ...\n",
    "     ...\n",
    "     [21]]\n",
    "```\n",
    "\n",
    ">Note any numpy function reference can viewed in here https://docs.scipy.org or Google It\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#vector example in python\n",
    "## Your Code here\n",
    "vector_a  = None # make a new row vector using np.array([0,10,-2.5,21])\n",
    "vector_b  = None # make a new column vector using np.array([[0],[10],[-2.5],[21]])\n",
    "## End\n",
    "print(\"Printing a Row Vector Value:\\n\", vector_a)\n",
    "print(\"Printing a Column Vector Value:\\n\", vector_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Expected Output:\n",
    "```\n",
    "    Printing a Row Vector Value:\n",
    "     [  0.   10.   -2.5  21. ]\n",
    "    Printing a Column Vector Value:\n",
    "     [[  0. ]\n",
    "     [ 10. ]\n",
    "     [ -2.5]\n",
    "     [ 21. ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A matrix is a 2-D array of numbers.\n",
    " Dimensions are denoted by N_rows x N_cols\n",
    " \n",
    " where,\n",
    "\n",
    "     N_rows is number of rows in a matrix\n",
    "     N_cols is number of columns in a matrix\n",
    "\n",
    " >Examples \n",
    " ```\n",
    " A = [[  0.   10.   -2.5  21. ]\n",
    "     [  0.   10.   -2.5  21. ]\n",
    "     [  0.   10.   -2.5  21. ]]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matrix example in python\n",
    "## Your Code here\n",
    "matrix_a = None #use np.array([[0,10,-9.5,21],[-9,10,-2.5,61],[5,10,-2.5,21]]) try different dimension and values\n",
    "## End\n",
    "print(\"Printing a 3 x 4 Matrix:\\n\",matrix_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Expected Output:\n",
    "```\n",
    "Printing a 3 x 4 Matrix:\n",
    " [[  0.   10.   -9.5  21. ]\n",
    " [ -9.   10.   -2.5  61. ]\n",
    " [  5.   10.   -2.5  21. ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are arrays with more than two dimensions.\n",
    "\n",
    "As a example you can imagine a 3-D objects as a tensor of stacked 2d matrices in a line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tensor example in python ------ run this cell\n",
    "np.random.seed(1) # used for making random value generation fixed according to time\n",
    "tensor_a = np.random.randn(3,2,4) # used to create a n dimensional array of random scalar values.\n",
    "# It can also be created using np.array. See the reference provided.\n",
    "print(\"Printing a tensor:\\n\",tensor_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you would have guessed that scalars are subset of vectors which are subset of matrices and similarly matrices are subset of tensors.\n",
    "\n",
    "Lets have look at some matrix operations\n",
    "\n",
    "#### Transpose of a matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transpose of a matrix is mirror of the matrix along its diagonal running from 1st element.\n",
    "\n",
    "Example:\n",
    "\n",
    "    A = [0 1                      transpose(A) = [0 1 2\n",
    "         1 1                                      1 1 3]\n",
    "         2 3]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python BroadCasting\n",
    "Numpy provides many advance operation on matrices.\n",
    "\n",
    "Broadcasting a great way for working with arrays\n",
    "\n",
    "Example\n",
    "\n",
    "```\n",
    "    A is a matrix =             [0 1 2\n",
    "                                 2 1 4]\n",
    "                     \n",
    "    A_broadcast   = A + 3    =  [3 4 5\n",
    "                                 5 4 7]\n",
    "                                 \n",
    "    Many such operations can be used in numpy see th reference provided.```\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Transpose Example run this cell\n",
    "matrix_b = np.array([[0,1],[1,1],[2,3]])\n",
    "matrix_b_transpose = matrix_b.T # .T is a numpy function used to make transpose of an n dimension array\n",
    "matrix_b_broadcasted = matrix_b - 5\n",
    "print(\"Matrix\")\n",
    "print(matrix_b)\n",
    "print(\"\\nMatrix Transpose\")\n",
    "print(matrix_b_transpose)\n",
    "print(\"\\nMatrix Broadcasted\")\n",
    "print(matrix_b_broadcasted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>\n",
    "\n",
    "### 2. Multiplication of matrices and vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to multiply also known as dot product of matrices the number of columns of first matrix and number of rows of second matrix must be equal.\n",
    "\n",
    "Example:\n",
    "\n",
    "    A = matrix of dimension 4x5\n",
    "    B = matrix of dimension 5x3\n",
    "    C = A.B returns C as a matrix of dimension 4x3\n",
    "        where,\n",
    "        C[i,j] = sum(A[i,k]*B[i,k]) on k \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiplication example\n",
    "A = np.array([[1,2,1],[1,0,1]])# dimension 2x3\n",
    "print(\"Dimensions of A\",A.shape)# ndarray.shape return dimensions of a matrix\n",
    "B = np.array([[1,2],[3,4],[5,6]])# dimension 3x2\n",
    "print(\"Dimensions of B\",B.shape)\n",
    "## Your Code here\n",
    "C = None # use np.dot(A,B) which returns a dot product of A and B of 2x2 dimension\n",
    "## End\n",
    "print(\"Matrix C is:\\n\",C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Expected Output:\n",
    "```\n",
    "Dimensions of A (2, 3)\n",
    "Dimensions of B (3, 2)\n",
    "Matrix C is:\n",
    " [[12 16]\n",
    " [ 6  8]]\n",
    "```    \n",
    "\n",
    "\n",
    "The matrix product follows some properties which are useful lets have a look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Product Properties\n",
    "\n",
    "**A.(B+C) = A.B + A.C  ** >Distributive \n",
    "\n",
    "**A.(B.C) = (A.B).C     ** >Associative \n",
    "\n",
    "**A.B   !=   B.A    ** >Not Commutative \n",
    "\n",
    "**(A.B)<sup>T</sup> = B<sup>T</sup>.A<sup>T</sup>  **\n",
    "\n",
    "> You can verify above given Properties in cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try verifying above properties here:\n",
    "## Your Code here\n",
    "A = None\n",
    "B = None\n",
    "C = None\n",
    "\n",
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>\n",
    "### 3. Identity and Inverse Matrices\n",
    "\n",
    "** Identity Matrix(I)** > A square matrix(nxn)  with diagonals as 1 and all other elements as zeros.\n",
    "\n",
    "**A.I = A** > Where I is identity matrix\n",
    "\n",
    "**A<sup>-1</sup>.A = I** > Inverse property of a matrix\n",
    "\n",
    "> **NOTE** Square matrices have No.-rows = No.-cols ==> NxN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to check above properties\n",
    "np.random.seed(2)\n",
    "matA = np.random.randn(3,3)\n",
    "matA_inv = np.linalg.inv(matA) # a method to invert an n-D square matrix\n",
    "matI = np.dot(matA_inv,matA)# note : numerically extremely small numbers may appear in places of zeros due to computation error\n",
    "matB = np.dot(matA,matI)\n",
    "print(\"Printing A:\\n\",matA)\n",
    "print(\"\\nPrinting A_inv:\\n\",matA_inv)\n",
    "print(\"\\nVerifying property 2 Inverse Multiplication:\\n\",matI)\n",
    "print(\"\\nVerifying property 1 Identity Multiplication:\\n\",matB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>\n",
    "### 4. Linear Dependence and span\n",
    "\n",
    "To Solve a Linear System of equation like below, In this equation n equation are placed with <=n number of unknown variables\n",
    "\n",
    "**A.X = B** >>Linear Combination of A and X\n",
    "\n",
    "where,\n",
    "X is a Vector with unknown variables and A is a Square Matrix. We can multiply inverse of matrix A on Both sides to get X.\n",
    "    \n",
    "**X = A<sup>-1</sup>.B**\n",
    "\n",
    "> NOTE: A square matrix need not have a inverse every time due to sigularity. Please read the reference on http://www.deeplearningbook.org/contents/linear_algebra.html Section 2.4\n",
    "\n",
    "The span of a set of vectors is the set of all points obtainable by linear combination of the original vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Combination problem\n",
    "#suppose you have two equations  2x + y = 1 and 5x + 9y = -4 how will you solve using matrices\n",
    "## Your Code here\n",
    "A =None #Hint: try to convert above two equation in form of matrix multiplication A.X = B\n",
    "B = None\n",
    "A_inv = np.linalg.inv(A)\n",
    "X = None\n",
    "#End\n",
    "print(\"The values of variables are:\\nX = \",X[0],\",Y = \",X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Expected Output\n",
    "```\n",
    "The values of variables are:\n",
    "X =  [ 1.] ,Y =  [-1.]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br></br><br></br>\n",
    "### 5. Norms\n",
    "Norms are a way of measuring size / length of a vector or you can say norms are functions to map vectors to a non negative values.\n",
    "\n",
    "The Mostly used norm definiton is as follows:\n",
    "\n",
    "**||X||<sub>p</sub> = [sum<sub>i</sub>(|X<sub>i</sub>|)<sup>p</sup>]<sup>1/p</sup>**\n",
    ">If p = 2 it's called euclidean distance or norm. If this norm is one for a vector then the vecot is called a unit vector\n",
    "\n",
    "The norm function can also be described as a function that satisfies these conditions:\n",
    "    1. f(X) = 0 --> X=0\n",
    "    2. f(X+Y) <= f(X) + f(Y)\n",
    "    3. f(aX) = |a|f(X) for all values of a\n",
    "    \n",
    "Examples:\n",
    " ```\n",
    "    max_norm(x) = max(|x|)\n",
    "    l1_norm(x)  = sum(|x|)\n",
    "    ```\n",
    "\n",
    "To measure size of a Matrix : **Frobeinius_norm**\n",
    "\n",
    "**||A||<sub>F</sub> = sqrt(sum(a<sub>i,j</sub>))**\n",
    "\n",
    "----------------------------------------\n",
    "\n",
    "**Determinant of Matrix**\n",
    "> To map a square matrix to a scalar value we ue something called determinant of a matrix. please look here its easy to understand how to calculate determinant https://en.wikipedia.org/wiki/Determinant/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Norm try and check with numpy l2_norm --> ||x|| , p = 2\n",
    "V = np.array([1,6,-1,2])\n",
    "np_norm = np.linalg.norm(V)\n",
    "## Calculate l2_norm = np.sqrt(np.sum(np.square(Vector)))\n",
    "## Your Code here\n",
    "my_norm = None\n",
    "#End\n",
    "print(\"numpy norm\",np_norm)\n",
    "print(\"your norm\",my_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Special Matrices and Vectors\n",
    "\n",
    "** Diagonal Matrix **\n",
    "\n",
    "Matrix that have all elements as zero other than diagonal elements.\n",
    "Example:\n",
    "\n",
    "    A = [2 0 0\n",
    "         0 5 0\n",
    "         0 0 3]\n",
    "         \n",
    "** Symmetric Matrix **\n",
    "\n",
    "Matrix having itself as its transpose.\n",
    "Example:\n",
    "\n",
    "    A = [2  0 -1\n",
    "         0  3  2\n",
    "        -1  2  3]\n",
    "        \n",
    "** Orthogonal Matrix **\n",
    "\n",
    "An orthogonal matrix is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal.which iplies its inverse is its transpose:\n",
    "\n",
    "A<sup>-1</sup> = A<sup>T</sup>\n",
    "\n",
    "A.A<sup>T</sup>= I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Eigen Decomposition\n",
    "\n",
    "While operating on larger datasets matrix decomposition can be very useful to extract meaningful data.\n",
    "\n",
    "One of the most widely used kinds of matrix decomposition is calledeigen-decomposition, in which we decompose a matrix into a set of eigenvectors andeigenvalues.\n",
    "\n",
    ">Applicable only for Square Matrices\n",
    "\n",
    "**Eigenvector, Eigenvalue ?**\n",
    "An Eigen Vector for a squarematrix **A** is vector **v** when multiplied with A scale in magnitude and does not change its direction\n",
    "\n",
    "    A.v = k*v, \n",
    "\n",
    "where **k** is a constant known as eigenvalue corresponding to right eigenvector **v**\n",
    "\n",
    "one can also find a left Eigenvector as follows\n",
    "\n",
    "    u.A = j*u, \n",
    "\n",
    "where **j** is a constant known as eigenvalue corresponding to left eigenvector **u**\n",
    "\n",
    ">NOTE: If we take every thing on above equation to right side and take determinant we will get **determinant(A-k.I) = 0**\n",
    "\n",
    "<br></br>\n",
    "\n",
    "**Decomposition ?**\n",
    "\n",
    "Say we have a set of eigenvectors{v1,v2......vn} and repective eigenvalues{k1,k2,.....kn} with respect to matrix A.\n",
    "Then we can stack them in matrix form so that:\n",
    "\n",
    "**A = V.diag(K).V<sup>-1</sup>**, Where the V is matrix of all eigenvectors diag(K) is a diagonal matrix containing all the\n",
    "eigen values.\n",
    "\n",
    "\n",
    "\n",
    "> Example:\n",
    "```\n",
    "A = [0  1        \n",
    "    -2 -3]\n",
    "by solving determinant(A-k.I) = 0 ==> k = -1 or -2 which are eigen values of A\n",
    "by putting back the eigen values into first equation we can get a set of two eigen vectors of unit magnitude\n",
    "v1 = [0.7071\n",
    "      -0.7071]\n",
    "v2 = [-0.4472\n",
    "      0.8944]     \n",
    "```\n",
    "For a quick tutorial look here:http://lpsa.swarthmore.edu/MtrxVibe/EigMat/MatrixEigen.html\n",
    "\n",
    "Not every matrix can be decomposed into eigenvalues and eigenvectors. In somecases, the decomposition exists but involves complex rather than real numbers.\n",
    "\n",
    "Speciﬁcally, every real symmetric matrix can be decomposed into an expression using only real-valued eigenvectors and eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Right Eigen example run this cell\n",
    "A = np.array([[0,1],[-2,-3]])\n",
    "w, v = np.linalg.eig(A) # used to compute eigen values and vectors for a matrix\n",
    "print(\"matrix A:\\n\", A)\n",
    "print(\"Eigen Values:\\n\",w)\n",
    "print(\"Eigen Vectors:\\n\",v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### 8. Singular Value Decompositon\n",
    "\n",
    "The singular value decomposition(SVD) provides another way to factorize a matrix, into singular vectors and singular values.\n",
    ">Applicable to all real matrices\n",
    "\n",
    "According to SVD every real matrix can be decomposed as follows:\n",
    "\n",
    "**A = U.D.V<sup>T</sup> **\n",
    "\n",
    "Where, \n",
    "```\n",
    "A is a m x n matrix, \n",
    "U is a m x m matrix, \n",
    "D is a m x n matrix and \n",
    "V is a n x n matrix.\n",
    "\n",
    "U and V are both Orthogonal Matrices\n",
    "D is a diagonal Matrix\n",
    "\n",
    "The elements along the diagonal of D are known as the singular values of the matrix A. The columns of U are known as the left-singular vectors. The columns of V are known as as the right-singular vectors.\n",
    "```\n",
    "By now u would have guessed that Eigen Decompostion is a special case of Sigular Value Decomposition.\n",
    "\n",
    ">**Calculation**:\n",
    ">>To gain more perspective on calculation of this matrices look in here:http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm\n",
    "\n",
    "The visualization of a SVD:  **A = UΣV<sup>T</sup>** can be presented as tranformation of data in following manner:\n",
    "<img src = \"linalg/svd.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Python SVD example\n",
    "A = np.array([[2,4],[1,0],[0,0],[0,0]])\n",
    "u, d, v = np.linalg.svd(A, full_matrices=1, compute_uv=1)# look the reference given for complete details about this funct.\n",
    "print(\"The matrix:\\n\",A)\n",
    "print(\"\\n\\nThe U matrix:\\n\",u)\n",
    "print(\"\\nThe D matrix:\\n\",d)\n",
    "print(\"\\nThe V matrix:\\n\",v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. The Moore-Penrose Pseudoinverse\n",
    "Inverse exists only for square matrices but there can be cases-\n",
    "\n",
    "when A is not a square matrix and following condition occurs:\n",
    ">A.x = y | x = B.y\n",
    "\n",
    "Then B is called a Pseudoinverse matrix of B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Trace Operator\n",
    "The sum of all diagonal elements of a matrix is defined as Trace of that matrix.\n",
    "\n",
    ">Example \n",
    "```\n",
    "A = [0 4 5\n",
    "     5 6 11\n",
    "     6 5 3]\n",
    "trace(A) = 0 + 6 + 3 = 9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Example : Principal Component Analysis\n",
    "Principal Component Analysis(PCA) is a machine learning algorithm which can be derived using only linear algebra knowledge that we have gained till now.\n",
    "\n",
    "Suppose we have a collection of m points {x(1), . . . , x(m)} in Rn and we want to apply lossy compression to these points to reduce storage memory requirement\n",
    "\n",
    "Lossy compression means storing the points in a way that requires less memory but may lose some precision. We want to lose as little precision as possible.\n",
    "Since these are n dimensional data we can reduce the number of dimension by eliminating the less meaningfull dimensions.\n",
    "\n",
    "In other words PCA is something that compresses a lot of data to esscence of the original data.\n",
    "\n",
    "----------------------\n",
    "**Redcuing dimenison using covariance matrix **\n",
    "\n",
    "Before we jump to PCA lets see some basic statistical parameters:\n",
    "\n",
    "For a dataset A = {x1,x2,.......xn} are all data points collected we say:\n",
    "\n",
    "** Mean(x<sub>m</sub>) = sum<sub>i</sub>(xi)/n** gives the average of the data\n",
    "\n",
    "** Variance(v<sub>A</sub>) = sum<sub>i</sub>((x-x<sub>m</sub>)<sup>2</sup>)/n**, tells about how varying the data is.\n",
    "\n",
    "** Standard Deviation(SD<sub>A</sub>) = sqrt(v<sub>A</sub>) **\n",
    "\n",
    "say there are two data sets A as same and B = {y1,y2......yn} then we can define covariance\n",
    "\n",
    "** Covariance(C<sub>AB</sub>) = sum<sub>i</sub>((x-x<sub>m</sub>)*(y-y<sub>m</sub>))/n**, tells about how A and B vary with respect to each other. In other words it tells us about co-relation between the two datasets. It says if C is very small then the data are orthogonal and not related to each other and if its very high then the data is highly related to each other.\n",
    "\n",
    "\n",
    ">Note: If you want to know more about these statistical parameters. Google (parameter) => Understood.\n",
    "\n",
    "Now,\n",
    "\n",
    "Suppose you have a n-dimensional dataset **X** with following dimension:: *d1,d2,d3,d4,d5,d6.*\n",
    "\n",
    "we are going to define a covariance matrix between these dimension:\n",
    "\n",
    "C_mat = <br>[<br>C<sub>11</sub>,C<sub>12</sub>,........,C<sub>16</sub><br>\n",
    "C<sub>21</sub>,C<sub>22</sub>,........,C<sub>26</sub><br>\n",
    ". . . . . . . . . . . . . .<br>\n",
    ". . . . . . . . . . . . . .<br>\n",
    "C<sub>61</sub>,C<sub>62</sub>,........,C<sub>66</sub><br>\n",
    "]\n",
    "\n",
    "In this matrix you would have noticed that for the  dimensional pairs if C<sub>ij</sub> is very high then the data dimensions are corelated and can be eliminated to reduce redundancy.\n",
    "\n",
    "Now,\n",
    "Compute the eigen vectors and eigen values of covariance matrix.\n",
    "\n",
    "Arrange them in descending order and remove the unwanted eigen value and respective eigen vector.\n",
    "\n",
    "Multiply the resulting vector matrix to the data atrix to get transformed data.\n",
    "\n",
    "\n",
    "-------------------\n",
    "\n",
    "**Reducing dimensions using SVD:**\n",
    "\n",
    "Start by calculating:\n",
    "\n",
    "X ==> U.D.V<sup>T</sup>\n",
    "\n",
    "Y = U<sup>T</sup>.X\n",
    "\n",
    "Now since U and V are orthogonal matrices you can see that :\n",
    "\n",
    "Z = Y.Y<sup>T</sup> \n",
    "<br>Z = U<sup>T</sup>.X.(U<sup>T</sup>.X)<sup>T</sup>\n",
    "<br>Z = (D.V).(V<sup>T</sup>.D<sup>T</sup>)\n",
    "<br>Z = D.D<sup>T</sup>\n",
    "\n",
    "If you have understood SVD calculation you can say Z is similar to corelation matrix between singular vectors U and the singular vectors are called the principal components.\n",
    "\n",
    "Now,\n",
    "to reduce the dimensions arrange values of Z in descending order and respectively the vectors in U.\n",
    "\n",
    "Decide number of vectors to keep by analyzing how much percentage does the value in Z adds to trace of Z which denotes hows much data is represented by corresponding vector in U and remove the unwanted vectors.\n",
    "Now that the you have your new PC Vectors calculate:\n",
    "\n",
    "X_new = U.Y\n",
    "\n",
    "\n",
    "We are going to create two calsses of 3D dimensional data and apply dimensionality reduction on them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run this cell to generate data for compression\n",
    "#Example Principal Component Analysis run this cell:\n",
    "%pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.mplot3d import proj3d\n",
    "\n",
    "# Generating data\n",
    "np.random.seed(107) # random seed for consistency\n",
    "mu_vec1 = np.array([0,0,0]) # mean value for creation of data\n",
    "cov_mat1 = np.array([[1,0,0],[0,1,0],[0,0,1]]) # covariance value for creation of data\n",
    "class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, 20).T # data fro class one\n",
    "assert class1_sample.shape == (3,20), \"class_1 dimension error\"\n",
    "\n",
    "# similarily for second set of class...................\n",
    "mu_vec2 = np.array([2,2,2])\n",
    "cov_mat2 = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "class2_sample = np.random.multivariate_normal(mu_vec2, cov_mat2, 20).T\n",
    "assert class2_sample.shape == (3,20), \"class_2 dimension error\"\n",
    "\n",
    "\n",
    "## plotting the genrated data\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "plt.rcParams['legend.fontsize'] = 10   \n",
    "ax.plot(class1_sample[0,:], class1_sample[1,:], class1_sample[2,:], 'o', markersize=8, color='blue', alpha=0.7, label='class1')\n",
    "ax.plot(class2_sample[0,:], class2_sample[1,:], class2_sample[2,:], 's', markersize=8, alpha=0.7, color='green', label='class2')\n",
    "plt.title('Datapoints of class 1 and class 2')\n",
    "ax.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#joining the datasets to get a new single data set\n",
    "data = np.concatenate((class1_sample, class2_sample), axis=1)\n",
    "assert data.shape == (3,40), \"joining dimensional error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA_Covariance\n",
    ">Try with the Covariance method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Complete this function to apply pca using covariance matrix\n",
    "def pca_cov(data):\n",
    "    #Compute the mean vector\n",
    "    mean_x = np.mean(data[0,:])\n",
    "    mean_y = np.mean(data[1,:])\n",
    "    mean_z = np.mean(data[2,:])\n",
    "    mean_vector = np.array([[mean_x],[mean_y],[mean_z]])\n",
    "    \n",
    "    #shifting data to set mean as origin\n",
    "    data = data-mean_vector\n",
    "\n",
    "    #computing covariance matrix\n",
    "    cov_mat = np.cov([data[0,:],data[1,:],data[2,:]])\n",
    "\n",
    "\n",
    "    #computing eigen values and vectors\n",
    "    ## Your Code here\n",
    "    eig_val, eig_vec = None #Compute eigen vectors and values using -->  np.linalg.eig(cov_mat)\n",
    "    #End\n",
    "    #sorting according to Decreasing eigen values\n",
    "    key = argsort(eig_val)[::-1]\n",
    "    eig_val, eig_vec = eig_val[key], eig_vec[:, key]\n",
    "\n",
    "    #reducing 3 dimensional data to 2 dimensions\n",
    "    pca = eig_vec[:,:2]\n",
    "\n",
    "    #calculating new transformed data\n",
    "    ## Your Code here\n",
    "    X_pca = None #calculate dot product of the new vector and data --> np.dot(pca.T,data)\n",
    "    #End\n",
    "    return X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA_SVD\n",
    ">Try with the SVD method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Complete this function to apply pca using svd\n",
    "def pca_svd(data):\n",
    "    #Compute the mean vector\n",
    "    mean_x = np.mean(data[0,:])\n",
    "    mean_y = np.mean(data[1,:])\n",
    "    mean_z = np.mean(data[2,:])\n",
    "    mean_vector = np.array([[mean_x],[mean_y],[mean_z]])\n",
    "    \n",
    "    #shifting data to set mean as origin\n",
    "    data = data-mean_vector\n",
    "\n",
    "    #computing svd\n",
    "    ## Your Code here\n",
    "    u, d, v = None # calculate the svd of data using np.linalg.svd(data, full_matrices=1, compute_uv=1)\n",
    "    #End\n",
    "    ut = u.T\n",
    "\n",
    "    #computing y and z\n",
    "    y = np.dot(u.T,data)\n",
    "    z = np.dot(y,y.T)\n",
    "\n",
    "    #sorting according to Decreasing eigen values\n",
    "    idx = np.diag(z)\n",
    "    key = argsort(idx)[::-1]\n",
    "    u_new = ut[:,key]\n",
    "    u_new = u_new[0:2,:]\n",
    "\n",
    "    #calculating new transformed data\n",
    "    X_pca = np.dot(u_new,data)\n",
    "    return X_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA sk_learn library\n",
    ">Try with the inbuilt sk_learn library method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparision with sklearn.decomposition library\n",
    "from sklearn.decomposition import PCA as PCAskl\n",
    "\n",
    "sklearn_pca = PCAskl(n_components=2)\n",
    "sklearn_transf = sklearn_pca.fit_transform(data.T)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#plottinng data\n",
    "plt.plot(sklearn_transf[0:20,0],sklearn_transf[0:20,1], 'o', markersize=7, color='blue', alpha=0.5, label='class1')\n",
    "plt.plot(sklearn_transf[20:40,0], sklearn_transf[20:40,1], 's', markersize=7, color='green', alpha=0.5, label='class2')\n",
    "plt.xlabel('pca-1')\n",
    "plt.ylabel('pca-2')\n",
    "plt.xlim([-4,4])\n",
    "plt.ylim([-4,4])\n",
    "plt.legend()\n",
    "plt.title('Transformed samples: Sk_learn library')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plotting the covariance pca\n",
    "X_pca = pca_cov(data[:])\n",
    "plt.plot(X_pca[0,0:20], X_pca[1,0:20], 'o', markersize=7, color='blue', alpha=0.5, label='class1')\n",
    "plt.plot(X_pca[0,20:40], X_pca[1,20:40], 's', markersize=7, color='green', alpha=0.5, label='class2')\n",
    "plt.xlim([-4,4])\n",
    "plt.ylim([-4,4])\n",
    "plt.xlabel('pca-1')\n",
    "plt.ylabel('pca-2')\n",
    "plt.legend()\n",
    "plt.title('Transformed samples: Covariance Method')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# plotting the svd pca\n",
    "X_pca = pca_svd(data[:])\n",
    "plt.plot(X_pca[0,0:20], X_pca[1,0:20], 'o', markersize=7, color='blue', alpha=0.5, label='class1')\n",
    "plt.plot(X_pca[0,20:40], X_pca[1,20:40], 's', markersize=7, color='green', alpha=0.5, label='class2')\n",
    "plt.xlim([-4,4])\n",
    "plt.ylim([-4,4])\n",
    "plt.xlabel('pca-1')\n",
    "plt.ylabel('pca-2')\n",
    "plt.legend()\n",
    "plt.title('Transformed samples: SVD Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the data has been compressed correctly the only difference is of orientation of projection.\n",
    "You can use pca for application like face recognition also.\n",
    "\n",
    ">**Congratulation on completing the first tutorial you are doing great **\n",
    "\n",
    "In the next part we will discuss Probability and Information Theory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

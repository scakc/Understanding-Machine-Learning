{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Training Deep Models\n",
    "\n",
    "Reference book: http://www.deeplearningbook.org/contents/optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If you are familier with following concept you can skip and move forwards:\n",
    "\n",
    "    1. How Learning Diﬀers from Pure Optimization\n",
    "    2. Challenges in Neural Network Optimization\n",
    "    3. Basic Algorithms\n",
    "    4. Parameter Initialization Strategies\n",
    "    5. Algorithms with Adaptive Learning Rates\n",
    "    6. Approximate Second-Order Methods\n",
    "    7. Optimization Strategies and Meta-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0, './opt')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ann1o as ann # the nueral network model we created in previous lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How Learning Diﬀers from Pure Optimization\n",
    "Includes\n",
    "    - Emperical Risk minimization.\n",
    "    - Surrogate loss function and early stopping.\n",
    "    - Batch and minibatch Algorithms.\n",
    "    \n",
    "> Read section 8.1 here http://www.deeplearningbook.org/contents/optimization.html\n",
    "\n",
    "Before proceeding to minibatch gradient descent code let's get familier with new structure of our previous model given in file ann10.py in opt folder.... let's check if model works...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will try to fit a quadratic data\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x260fea30cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# defining the data to fit.......\n",
    "np.random.seed(18)\n",
    "n_points = 80\n",
    "# we are limiting the output to 0 to one since the output node gives probability between 0 to 1\n",
    "X = np.linspace(0,0.8,n_points).reshape(n_points,1)\n",
    "np.random.shuffle(X)\n",
    "X = X.T + 0.2\n",
    "Y = abs(X**2)+np.random.randn(1,n_points)*0.01  + 0.3\n",
    "Y = 0.8*Y/np.max(Y)\n",
    "print(\"We will try to fit a quadratic data\")\n",
    "# defining training data\n",
    "tr_size = 60\n",
    "x_tr = X[0,:tr_size].reshape(1,tr_size)\n",
    "y_tr = Y[0,:tr_size].reshape(1,tr_size)\n",
    "x_te = X[0,tr_size:].reshape(1,n_points-tr_size)\n",
    "y_te = Y[0,tr_size:].reshape(1,n_points-tr_size)\n",
    "# defining validation data\n",
    "str_size = 50\n",
    "x_str = x_tr[0,:str_size].reshape(1,str_size)\n",
    "y_str = y_tr[0,:str_size].reshape(1,str_size)\n",
    "x_val = x_tr[0,str_size:].reshape(1,tr_size-str_size)\n",
    "y_val = y_tr[0,str_size:].reshape(1,tr_size-str_size)\n",
    "plt.clf()\n",
    "plt.plot(x_str.T,y_str.T,'x',label = 'training')\n",
    "plt.plot(x_val.T,y_val.T,'x',label = 'validation')\n",
    "plt.plot(x_te.T,y_te.T,'x',label = 'testing')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 150000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 10000, \n",
    "    'plot_cost'   : True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann1 = ann.NN(layers, actis, hyperparams)\n",
    "ann1.train(x_str, y_str, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = ann1.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use classes helps us to extend or scale our model freely so<br>\n",
    "Now we can start minibatch gradient descent ........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minibatch gradient descent\n",
    "class mbatch_NN(ann.NN):\n",
    "    \n",
    "    def create_batches(self, x, y, k):\n",
    "        [n,m] = x.shape\n",
    "        self.batches = {}\n",
    "        siz = m//k\n",
    "        perm = np.random.permutation(m)\n",
    "        xd = x[:,perm]\n",
    "        yd = y[:,perm]\n",
    "        \n",
    "        for i in range(k):\n",
    "            if (i == k-1):\n",
    "                self.batches['bx'+ str(i)] = xd[:,i*siz:].reshape(n,-1)\n",
    "                self.batches['by'+ str(i)] = yd[:,i*siz:].reshape(n,-1)\n",
    "            else:\n",
    "                self.batches['bx'+ str(i)] = xd[:,i*siz:(i+1)*siz].reshape(n,siz)\n",
    "                self.batches['by'+ str(i)] = yd[:,i*siz:(i+1)*siz].reshape(n,siz)\n",
    "                \n",
    "    def train_batches(self, x_str,y_str,xval,yval, k):\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        \n",
    "        self.create_batches(x_str, y_str, k)\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            for j in range(k):\n",
    "                \n",
    "                X = self.batches['bx'+ str(j)]\n",
    "                Y = self.batches['by'+ str(j)]\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                if (self.plotcost):\n",
    "                    cost = self.compcost(a3, Y)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "\n",
    "                self.parameters = self.updateparams(grads, self.ld / m)\n",
    "                \n",
    "                \n",
    "            if (self.plotcost):\n",
    "                if self.print_cost and i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "            \n",
    "        self.costs.append(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 150000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 10000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann2 = mbatch_NN(layers, actis, hyperparams)\n",
    "ann2.train_batches(x_str, y_str, x_val, y_val, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = ann2.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Challenges in Neural Network Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would ask you to read section 8.2 from the book although it's all theoritical it includes the pitfalls of nueral network optimization which can save you in pinch so please read section 8.2 here http://www.deeplearningbook.org/contents/optimization.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Basic Algorithms\n",
    "\n",
    "#### 3.1 Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# minibatch gradient descent\n",
    "class stochastic_NN(ann.NN):\n",
    "              \n",
    "    def train_stochastic(self, x_str,y_str,xval,yval, batch_size=1, num_iter = None, init_lr = None):\n",
    "        if(num_iter == None):\n",
    "            num_iter = self.num_epochs\n",
    "        if(init_lr == None):\n",
    "            init_lr = self.learning_rate\n",
    "            \n",
    "            \n",
    "        self.learning_rate = init_lr\n",
    "        self.num_epochs = num_iter\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        olddiff = 0\n",
    "        diff = 0\n",
    "        mincost = valcst\n",
    "        \n",
    "        knb = m//batch_size\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            \n",
    "            for j in range(knb):\n",
    "                \n",
    "                lb = j*batch_size\n",
    "                ub = min(m,(j+1)*batch_size)\n",
    "                siz = ub-lb\n",
    "                X = x_str[:,lb:ub].reshape(n,siz)\n",
    "                Y = y_str[:,lb:ub].reshape(n,siz)\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "\n",
    "                self.parameters = self.updateparams(grads, self.ld / m)\n",
    "            \n",
    "            cost = self.compcost(a3, Y)\n",
    "            \n",
    "            if (mincost > cost):\n",
    "                mincost = cost\n",
    "                olddiff = diff\n",
    "                diff = cost-mincost\n",
    "                self.learning_rate = min(self.learning_rate*(np.sign(olddiff-diff)*0.01+1),0.9)\n",
    "            else:\n",
    "                olddiff = diff\n",
    "                diff = cost-mincost\n",
    "                self.learning_rate = min(self.learning_rate*(np.sign(olddiff-diff)*0.01+1),0.9)\n",
    "            \n",
    "                \n",
    "            if (self.print_cost):\n",
    "                if i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration\", i, ' is:', cost,':: l_rate',self.learning_rate)\n",
    "            if (self.plotcost):\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "            \n",
    "        self.costs.append(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 1000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann3 = stochastic_NN(layers, actis, hyperparams)\n",
    "ann3.train_stochastic(x_str, y_str, x_val, y_val,batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = ann3.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Momentum  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class momentum_NN(ann.NN):\n",
    "    def init_velocity(self):\n",
    "        self.velocity = {}\n",
    "        for l in range(1, self.L):\n",
    "            self.velocity['W' + str(l)] = self.parameters['W' + str(l)]*0\n",
    "            self.velocity['b' + str(l)] = self.parameters['b' + str(l)]*0\n",
    "            \n",
    "    def update_velocity(self, grads, ldbym, alpha):\n",
    "        L = len(self.parameters) // 2  # number of layers in the neural network\n",
    "        \n",
    "        # Update rule for each parameter. Use a for loop.\n",
    "        for l in range(L):\n",
    "            \n",
    "            if (self.reg == 'l2reg'):\n",
    "                self.velocity[\"W\" + str(l+1)] = alpha*self.velocity[\"W\" + str(l+1)] + (1-alpha)*(grads[\n",
    "                    \"dW\" + str(l + 1)] + ldbym*self.parameters[\"W\" + str(l + 1)])\n",
    "                \n",
    "            elif (self.reg == 'l1reg'):\n",
    "                self.velocity[\"W\" + str(l+1)] = alpha*self.velocity[\"W\" + str(l+1)]\n",
    "                - (1-alpha)*self.learning_rate*(grads[\"dW\" + str(l + 1)] + ldbym*np.sign(self.parameters[\"W\" + str(l + 1)]))\n",
    "                \n",
    "            else:\n",
    "                self.velocity[\"W\" + str(l+1)] = alpha*self.velocity[\"W\" + str(l+1)] + (1-alpha)*(grads[\n",
    "                    \"dW\" + str(l + 1)])   \n",
    "                                                                                                          \n",
    "            self.velocity[\"b\" + str(l+1)] = alpha*self.velocity[\"b\" + str(l+1)] + (1-alpha)*(grads[\n",
    "                    \"db\" + str(l + 1)]) \n",
    "            \n",
    "        l = 0\n",
    "        for l in range(L):\n",
    "            assert(self.velocity[\"W\" + str(l+1)].shape == self.parameters[\"W\" + str(l+1)].shape)\n",
    "            self.parameters[\"W\" + str(l+1)] = self.parameters[\"W\" + str(l+1)] - self.learning_rate*self.velocity[\"W\" + str(l+1)]\n",
    "            self.parameters[\"b\" + str(l+1)] = self.parameters[\"b\" + str(l+1)] - self.learning_rate*self.velocity[\"b\" + str(l+1)]\n",
    "                                                                                                          \n",
    "        return self.parameters\n",
    "    \n",
    "    def train_momentum(self, x_str,y_str,xval,yval, batch_size=1, num_iter = None, init_lr = None, alpha = 0.9):\n",
    "       \n",
    "        self.init_velocity()\n",
    "        \n",
    "        if(num_iter == None):\n",
    "            num_iter = self.num_epochs\n",
    "        if(init_lr == None):\n",
    "            init_lr = self.learning_rate\n",
    "            \n",
    "            \n",
    "        self.learning_rate = init_lr\n",
    "        self.num_epochs = num_iter\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        olddiff = 0\n",
    "        diff = 0\n",
    "        mincost = valcst\n",
    "        \n",
    "        knb = m//batch_size\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            \n",
    "            for j in range(knb):\n",
    "                \n",
    "                lb = j*batch_size\n",
    "                ub = min(m,(j+1)*batch_size)\n",
    "                siz = ub-lb\n",
    "                X = x_str[:,lb:ub].reshape(n,siz)\n",
    "                Y = y_str[:,lb:ub].reshape(n,siz)\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "                \n",
    "                self.parameters = self.update_velocity(grads, self.ld / m, alpha)\n",
    "            \n",
    "            cost = self.compcost(a3, Y)\n",
    "            \n",
    "            if (mincost > cost):\n",
    "                mincost = cost\n",
    "                olddiff = diff\n",
    "                diff = cost-mincost\n",
    "                self.learning_rate = min(self.learning_rate*(np.sign(olddiff-diff)*0.01+1),0.9)\n",
    "            else:\n",
    "                olddiff = diff\n",
    "                diff = cost-mincost\n",
    "                self.learning_rate = min(self.learning_rate*(np.sign(olddiff-diff)*0.01+1),0.9)\n",
    "            \n",
    "                \n",
    "            if (self.print_cost):\n",
    "                if i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration\", i, ' is:', cost,':: l_rate',self.learning_rate)\n",
    "            if (self.plotcost):\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "        \n",
    "        self.costs.append(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 1000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann4 = momentum_NN(layers, actis, hyperparams)\n",
    "ann4.train_momentum(x_str, y_str, x_val, y_val,batch_size = 3, alpha = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = ann4.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----------------------\n",
    "### 4. Parameter Initialization Strategies\n",
    "    \n",
    "   1. Xavier's Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class init_NN(ann.NN):\n",
    "    def xav_init_params(self):\n",
    "        np.random.seed(3)\n",
    "        for l in range(1, self.L):\n",
    "            a = np.sqrt(2/(self.layers[l] + self.layers[l - 1]))\n",
    "            self.parameters['W' + str(l)] = np.random.uniform(-a,a,size=[self.layers[l], self.layers[l - 1]])\n",
    "            self.parameters['b' + str(l)] = np.zeros((self.layers[l], 1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 100, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann5 = init_NN(layers, actis, hyperparams)\n",
    "ann5.train(x_str, y_str, x_val, y_val)\n",
    "cost1 = ann5.costs\n",
    "pY1 = ann5.predict(x_te)\n",
    "ann5.costs = []\n",
    "ann5.xav_init_params()\n",
    "ann5.train(x_str, y_str, x_val, y_val)\n",
    "cost2 = ann5.costs\n",
    "pY2 = ann5.predict(x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.squeeze(cost1))\n",
    "plt.plot(np.squeeze(cost2))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations /' + str(ann5.printerval))\n",
    "plt.title(\"Learning rate =\" + str(ann5.learning_rate))\n",
    "plt.show()\n",
    "print(\"Predicted fit.....\")\n",
    "err1 = np.sum((pY1 - y_te)**2)/y_te.shape[1]\n",
    "err2 = np.sum((pY2 - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE1 is : \", err1)\n",
    "print(\"The MSE2 is : \", err2)\n",
    "plt.plot(x_te.T,pY1.T,'.')\n",
    "plt.plot(x_te.T,pY2.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Algorithms with Adaptive Learning Rates\n",
    "\n",
    "#### 5.1 AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class adagrad_NN(ann.NN):\n",
    "    def update_adagrad(self, grads, ldbym):\n",
    "        ep = 10**-8\n",
    "        L = len(self.parameters) // 2  # number of layers in the neural network\n",
    "        # Update rule for each parameter. Use a for loop.\n",
    "        for l in range(L):\n",
    "            if (self.reg == 'l2reg'):\n",
    "                grads[\"dW\" + str(l + 1)] = grads[\"dW\" + str(l + 1)] + ldbym*self.parameters[\"W\" + str(l + 1)]\n",
    "            elif (self.reg == 'l1reg'):\n",
    "                grads[\"dW\" + str(l + 1)] = grads[\"dW\" + str(l + 1)] + ldbym*np.sign(self.parameters[\"W\" + str(l + 1)])\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "            self.R['W'+str(l+1)] = self.R['W'+str(l+1)] + grads['dW'+str(l+1)]**2\n",
    "            deno = np.sqrt(self.R['W' + str(l+1)] + ep)\n",
    "            \n",
    "            self.parameters[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)] - (self.learning_rate/deno) * grads[\"dW\" + str(l + 1)]\n",
    "\n",
    "            self.R['b'+str(l+1)] = self.R['b'+str(l+1)] + grads['db'+str(l+1)]**2\n",
    "            deno = np.sqrt(self.R['b' + str(l+1)] + ep)\n",
    "            self.parameters[\"b\" + str(l + 1)] = self.parameters[\"b\" + str(l + 1)] - (self.learning_rate/deno) * grads[\"db\" + str(l + 1)]\n",
    "            \n",
    "        return self.parameters\n",
    "    \n",
    "    def train_adagrad(self, x_str,y_str,xval,yval, batch_size=None, num_iter = None, init_lr = None):\n",
    "        self.R = {}\n",
    "        for l in range(1, self.L):\n",
    "            self.R['W' + str(l)] = np.zeros((self.layers[l], self.layers[l - 1]))\n",
    "            self.R['b' + str(l)] = np.zeros((self.layers[l], 1))\n",
    "            \n",
    "        \n",
    "        if(num_iter == None):\n",
    "            num_iter = self.num_epochs\n",
    "        if(init_lr == None):\n",
    "            init_lr = self.learning_rate\n",
    "        if(batch_size == None):\n",
    "            batch_size = x_str.shape[1]\n",
    "            \n",
    "            \n",
    "        self.learning_rate = init_lr\n",
    "        self.num_epochs = num_iter\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        olddiff = 0\n",
    "        diff = 0\n",
    "        mincost = valcst\n",
    "        \n",
    "        knb = m//batch_size\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            \n",
    "            for j in range(knb):\n",
    "                \n",
    "                lb = j*batch_size\n",
    "                ub = min(m,(j+1)*batch_size)\n",
    "                siz = ub-lb\n",
    "                X = x_str[:,lb:ub].reshape(n,siz)\n",
    "                Y = y_str[:,lb:ub].reshape(n,siz)\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "\n",
    "                self.parameters = self.update_adagrad(grads, self.ld / m)\n",
    "            \n",
    "            cost = self.compcost(a3, Y)\n",
    "        \n",
    "            \n",
    "                \n",
    "            if (self.print_cost):\n",
    "                if i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration\", i, ' is:', cost)\n",
    "            if (self.plotcost):\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "            \n",
    "        self.costs.append(costs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.9,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 1000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann6 = adagrad_NN(layers, actis, hyperparams)\n",
    "ann6.train_adagrad(x_str, y_str, x_val, y_val, batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = ann6.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 AdaDelta / RmsProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class rmsprop_NN(ann.NN):\n",
    "    def update_rmsprop(self, grads, ldbym):\n",
    "        ep = 10**-6\n",
    "        L = len(self.parameters) // 2  # number of layers in the neural network\n",
    "        # Update rule for each parameter. Use a for loop.\n",
    "        for l in range(L):\n",
    "            if (self.reg == 'l2reg'):\n",
    "                grads[\"dW\" + str(l + 1)] = grads[\"dW\" + str(l + 1)] + ldbym*self.parameters[\"W\" + str(l + 1)]\n",
    "            elif (self.reg == 'l1reg'):\n",
    "                grads[\"dW\" + str(l + 1)] = grads[\"dW\" + str(l + 1)] + ldbym*np.sign(self.parameters[\"W\" + str(l + 1)])\n",
    "            else:\n",
    "                pass\n",
    "             \n",
    "            \n",
    "            self.R['W'+str(l+1)] = self.rho*self.R['W'+str(l+1)] + (1-self.rho)*grads['dW'+str(l+1)]**2\n",
    "            deno = np.sqrt(self.R['W' + str(l+1)]) + ep\n",
    "           \n",
    "            self.parameters[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)] - (self.learning_rate/deno) * grads[\"dW\" + str(l + 1)]\n",
    "\n",
    "            self.R['b'+str(l+1)] = self.R['b'+str(l+1)] + grads['db'+str(l+1)]**2\n",
    "            deno = np.sqrt(self.R['b' + str(l+1)]) + ep\n",
    "            self.parameters[\"b\" + str(l + 1)] = self.parameters[\"b\" + str(l + 1)] - (self.learning_rate/deno) * grads[\"db\" + str(l + 1)]\n",
    "            \n",
    "        return self.parameters\n",
    "    \n",
    "    def train_rmsprop(self, x_str,y_str,xval,yval,rho = 0.5, batch_size=None, num_iter = None, init_lr = None):\n",
    "        self.R = {}\n",
    "        self.rho = rho\n",
    "        for l in range(1, self.L):\n",
    "            self.R['W' + str(l)] = np.zeros((self.layers[l], self.layers[l - 1]))\n",
    "            self.R['b' + str(l)] = np.zeros((self.layers[l], 1))\n",
    "            \n",
    "        \n",
    "        if(num_iter == None):\n",
    "            num_iter = self.num_epochs\n",
    "        if(init_lr == None):\n",
    "            init_lr = self.learning_rate\n",
    "        if(batch_size == None):\n",
    "            batch_size = x_str.shape[1]\n",
    "            \n",
    "            \n",
    "        self.learning_rate = init_lr\n",
    "        self.num_epochs = num_iter\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        olddiff = 0\n",
    "        diff = 0\n",
    "        mincost = valcst\n",
    "        \n",
    "        knb = m//batch_size\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            \n",
    "            for j in range(knb):\n",
    "                \n",
    "                lb = j*batch_size\n",
    "                ub = min(m,(j+1)*batch_size)\n",
    "                siz = ub-lb\n",
    "                X = x_str[:,lb:ub].reshape(n,siz)\n",
    "                Y = y_str[:,lb:ub].reshape(n,siz)\n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "\n",
    "                self.parameters = self.update_rmsprop(grads, self.ld / m)\n",
    "            \n",
    "            cost = self.compcost(a3, Y)\n",
    "        \n",
    "            \n",
    "                \n",
    "            if (self.print_cost):\n",
    "                if i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration\", i, ' is:', cost)\n",
    "            if (self.plotcost):\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "            \n",
    "        self.costs.append(costs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.002,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 1000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann7 = rmsprop_NN(layers, actis, hyperparams)\n",
    "ann7.train_rmsprop(x_str, y_str, x_val, y_val, rho = 0.99, batch_size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = ann7.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 5.3 Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class adam_NN(ann.NN):\n",
    "    def init_velocity(self):\n",
    "        self.velocity = {}\n",
    "        for l in range(1, self.L):\n",
    "            self.velocity['W' + str(l)] = self.parameters['W' + str(l)]*0\n",
    "            self.velocity['b' + str(l)] = self.parameters['b' + str(l)]*0\n",
    "            \n",
    "    def update_adam(self, grads, ldbym, alpha):\n",
    "        L = len(self.parameters) // 2  # number of layers in the neural network\n",
    "        ep = 10**-6\n",
    "        # Update rule for each parameter. Use a for loop.\n",
    "        for l in range(L):\n",
    "            \n",
    "            if (self.reg == 'l2reg'):\n",
    "                grads[\"dW\" + str(l + 1)] = grads[\"dW\" + str(l + 1)] + ldbym*self.parameters[\"W\" + str(l + 1)]\n",
    "            elif (self.reg == 'l1reg'):\n",
    "                grads[\"dW\" + str(l + 1)] = grads[\"dW\" + str(l + 1)] + ldbym*np.sign(self.parameters[\"W\" + str(l + 1)])\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            self.R['W'+str(l+1)] = (self.rho*self.R['W'+str(l+1)] + (1-self.rho)*grads['dW'+str(l+1)]**2)\n",
    "            self.cR['W'+str(l+1)] = self.R['W'+str(l+1)]/(1-self.rho**self.itern)\n",
    "            self.R['b'+str(l+1)] = (self.rho*self.R['b'+str(l+1)] + (1-self.rho)*grads['db'+str(l+1)]**2)\n",
    "            self.cR['b'+str(l+1)] = self.R['b'+str(l+1)]/(1-self.rho**self.itern)                        \n",
    "            \n",
    "            self.velocity[\"W\" + str(l+1)] = (alpha*self.velocity[\"W\" + str(l+1)] + (1-alpha)*(grads[\n",
    "                    \"dW\" + str(l + 1)]))\n",
    "            self.cV['W'+str(l+1)] = self.velocity['W'+str(l+1)]/(1-alpha**self.itern)\n",
    "                                                                                                          \n",
    "            self.velocity[\"b\" + str(l+1)] = (alpha*self.velocity[\"b\" + str(l+1)] + (1-alpha)*(grads[\n",
    "                    \"db\" + str(l + 1)]))\n",
    "            self.cV['b'+str(l+1)] = self.velocity['b'+str(l+1)]/(1-alpha**self.itern)\n",
    "            \n",
    "        l = 0\n",
    "        for l in range(L):\n",
    "            assert(self.velocity[\"W\" + str(l+1)].shape == self.parameters[\"W\" + str(l+1)].shape)\n",
    "            mul = self.learning_rate/(ep + np.sqrt(self.cR['W'+str(l+1)]))\n",
    "            self.parameters[\"W\" + str(l+1)] = self.parameters[\"W\" + str(l+1)] - mul*self.cV[\"W\" + str(l+1)]\n",
    "            mul = self.learning_rate/(ep + np.sqrt(self.cR['b'+str(l+1)]))\n",
    "            self.parameters[\"b\" + str(l+1)] = self.parameters[\"b\" + str(l+1)] - mul*self.cV[\"b\" + str(l+1)]\n",
    "                                                                                                          \n",
    "        return self.parameters\n",
    "    \n",
    "    def train_adam(self, x_str,y_str,xval,yval, batch_size=1, num_iter = None, init_lr = None, alpha = 0.99, rho = 0.9):\n",
    "        self.itern = 0\n",
    "        self.init_velocity()\n",
    "        self.R = {}\n",
    "        self.rho = rho\n",
    "        for l in range(1, self.L):\n",
    "            self.R['W' + str(l)] = np.zeros((self.layers[l], self.layers[l - 1]))\n",
    "            self.R['b' + str(l)] = np.zeros((self.layers[l], 1))\n",
    "              \n",
    "        self.cR = self.R\n",
    "        self.cV = self.velocity\n",
    "            \n",
    "        if(num_iter == None):\n",
    "            num_iter = self.num_epochs\n",
    "        if(init_lr == None):\n",
    "            init_lr = self.learning_rate\n",
    "            \n",
    "            \n",
    "        self.learning_rate = init_lr\n",
    "        self.num_epochs = num_iter\n",
    "        np.random.seed(1)\n",
    "        costs = []  # keep track of cost\n",
    "\n",
    "        [n, m] = x_str.shape\n",
    "\n",
    "        valcst = 10 ** 5\n",
    "        passes = 0\n",
    "        oparams = self.parameters\n",
    "        olddiff = 0\n",
    "        diff = 0\n",
    "        mincost = valcst\n",
    "        \n",
    "        knb = m//batch_size\n",
    "        \n",
    "        for i in range(0, self.num_epochs):\n",
    "            \n",
    "            for j in range(knb):\n",
    "                self.itern = self.itern + 1\n",
    "                lb = j*batch_size\n",
    "                ub = min(m,(j+1)*batch_size)\n",
    "                siz = ub-lb\n",
    "                X = x_str[:,lb:ub].reshape(n,siz)\n",
    "                Y = y_str[:,lb:ub].reshape(n,siz)\n",
    "                \n",
    "                a3, caches, masks = self.lmodelfwd(X)\n",
    "\n",
    "                grads = self.lmodelback(a3, Y, caches, masks)\n",
    "                \n",
    "                self.parameters = self.update_adam(grads, self.ld / m, alpha)\n",
    "                \n",
    "                \n",
    "            cost = self.compcost(a3, Y)\n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "            if (self.print_cost):\n",
    "                if i % self.printerval == 0:\n",
    "                    print(\"Cost after iteration\", i, ' is:', cost,':: l_rate',self.learning_rate)\n",
    "            if (self.plotcost):\n",
    "                if i % self.printerval == 0:\n",
    "                    costs.append(cost)\n",
    "\n",
    "            if (self.estop and i % 10 == 0):\n",
    "                yvald = self.predict(xval)\n",
    "                vcost = self.compcost(yvald, yval)\n",
    "                if (vcost < valcst):\n",
    "                    passes = 0\n",
    "                    valcst = vcost\n",
    "                    oparams = self.parameters\n",
    "                else:\n",
    "                    if (passes > self.max_passes):\n",
    "                        self.parameters = oparams\n",
    "                        print(\"breaking the loop........\")\n",
    "                        break\n",
    "                    else:\n",
    "                        passes = passes + 1\n",
    "\n",
    "        # plot the cost\n",
    "        if (self.plotcost):\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.ylabel('cost')\n",
    "            plt.xlabel('iterations /' + str(self.printerval))\n",
    "            plt.title(\"Learning rate =\" + str(self.learning_rate))\n",
    "            plt.show()\n",
    "        \n",
    "        self.costs.append(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [1,15,1]  # layer structure\n",
    "actis = ['none','relu','sigmoid'] # activation at each layer\n",
    "hyperparams = {\n",
    "    'learning_rate': 0.02,\n",
    "    'structural_reg' : 'dout',\n",
    "    'structural_lambda' : 0.5, \n",
    "    'lambdaa' : 0.001,\n",
    "    'regulization' : 'none', \n",
    "    'early_stop' : True, \n",
    "    'max_passes' : 1000, \n",
    "    'max_epochs' : 20000, \n",
    "    'display_cost' : True, \n",
    "    'cost_interval' : 1000, \n",
    "    'plot_cost'   : True,\n",
    "}\n",
    "\n",
    "ann8 = adam_NN(layers, actis, hyperparams)\n",
    "ann8.train_adam(x_str, y_str, x_val, y_val, batch_size = 5, alpha = 0.9, rho = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pY = ann8.predict(x_te)\n",
    "print(\"Predicted fit.....\")\n",
    "err = np.sum((pY - y_te)**2)/y_te.shape[1]\n",
    "print(\"The MSE is : \", err)\n",
    "plt.plot(x_te.T,pY.T,'.')\n",
    "plt.plot(x_te.T,y_te.T,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------\n",
    "### 6. Approximate Second-Order Methods\n",
    "\n",
    "Read Section 8.6 from book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------\n",
    "\n",
    "### 7. Optimization Strategies and Meta-Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
